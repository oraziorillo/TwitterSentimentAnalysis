{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaner for the extra dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c63936569873>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclean_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/github/twitter_sentiment_analysis/clean_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextblob\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTextBlob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m   \u001b[0;31m# This lemmatizer I hope works better than the TextBlob one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from clean_helpers import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams, \\\n",
    "            build_unique_ngrams, create_sentence_vectors, create_sentence_vectors_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\": clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive,\n",
    "    \"gensim_simple\": gensim_clean,   # not a good idea to use it I think! It cleans everything which is not alphabetic (special char, numbers and so on)\n",
    "    \"more_than_double_rep\": clean_more_than_double_repeated_chars,\n",
    "    \"clean_spelling\": clean_spelling,\n",
    "    \"lemmatize_spacy\": lemmatize_spacy,\n",
    "    \"remove_ats\": remove_ats,\n",
    "    \"remove_urls\": remove_urls,\n",
    "    \"remove_ampersand\": remove_ampersand,\n",
    "    \"clean_empty_sentences\": clean_empty_sentences\n",
    "}\n",
    "\n",
    "cleaning_options = ['clean_new_line', 'lowercase', 'clean_punctuation_2', 'clean_tags', 'remove_numbers', \n",
    "                    'lemmatize_spacy', 'more_than_double_rep', 'clean_empty_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/extra_data.csv\", usecols=range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'] = df['SentimentText']\n",
    "df = df.drop([\"SentimentText\"], axis=1)\n",
    "df['label'] = df['Sentiment']\n",
    "df = df.drop([\"Sentiment\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578609</td>\n",
       "      <td>1578623</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzz.... Finally! Night tweeters!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578610</td>\n",
       "      <td>1578624</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzzz, sleep well people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578611</td>\n",
       "      <td>1578625</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZzzZzZzzzZ... wait no I have homework.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578612</td>\n",
       "      <td>1578626</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>ZzZzzzZZZZzzz meh, what am I doing up again?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1578613</td>\n",
       "      <td>1578627</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>Zzzzzzzzzzzzzzzzzzz, I wish</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1578614 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ItemID SentimentSource  \\\n",
       "0              1    Sentiment140   \n",
       "1              2    Sentiment140   \n",
       "2              3    Sentiment140   \n",
       "3              4    Sentiment140   \n",
       "4              5    Sentiment140   \n",
       "...          ...             ...   \n",
       "1578609  1578623    Sentiment140   \n",
       "1578610  1578624    Sentiment140   \n",
       "1578611  1578625    Sentiment140   \n",
       "1578612  1578626    Sentiment140   \n",
       "1578613  1578627    Sentiment140   \n",
       "\n",
       "                                                  sentence  label  \n",
       "0                             is so sad for my APL frie...      0  \n",
       "1                           I missed the New Moon trail...      0  \n",
       "2                                  omg its already 7:30 :O      1  \n",
       "3                  .. Omgaga. Im sooo  im gunna CRy. I'...      0  \n",
       "4                 i think mi bf is cheating on me!!!   ...      0  \n",
       "...                                                    ...    ...  \n",
       "1578609               Zzzzzz.... Finally! Night tweeters!       1  \n",
       "1578610                        Zzzzzzz, sleep well people       1  \n",
       "1578611            ZzzZzZzzzZ... wait no I have homework.       0  \n",
       "1578612      ZzZzzzZZZZzzz meh, what am I doing up again?       0  \n",
       "1578613                       Zzzzzzzzzzzzzzzzzzz, I wish       0  \n",
       "\n",
       "[1578614 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56 s, sys: 216 ms, total: 56.2 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "from clean_helpers import *\n",
    "%time df_cleaned = tokenize_tweets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 s, sys: 100 ms, total: 2.71 s\n",
      "Wall time: 2.7 s\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0                    is so sad for my APL friend ...      0\n",
      "1                  I missed the New Moon trailer ...      0\n",
      "2                           omg its already 7:30 : O      1\n",
      "3  .. Omgaga . Im sooo im gunna CRy . I've been a...      0\n",
      "4          i think mi bf is cheating on me ! ! ! T_T      0\n",
      "unique words = 857776\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 4.43 s, sys: 56.1 ms, total: 4.49 s\n",
      "Wall time: 4.49 s\n",
      "lowercase\n",
      "                                            sentence  label\n",
      "0                    is so sad for my apl friend ...      0\n",
      "1                  i missed the new moon trailer ...      0\n",
      "2                           omg its already 7:30 : o      1\n",
      "3  .. omgaga . im sooo im gunna cry . i've been a...      0\n",
      "4          i think mi bf is cheating on me ! ! ! t_t      0\n",
      "unique words = 749815\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 2min 4s, sys: 464 ms, total: 2min 4s\n",
      "Wall time: 2min 4s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0                    is so sad for my apl friend ...      0\n",
      "1                  i missed the new moon trailer ...      0\n",
      "2                            omg it already 7:30 : o      1\n",
      "3  .. omgaga . im sooo im gunna cry . i've been a...      0\n",
      "4          i think mi bf is cheating on me ! ! ! t_t      0\n",
      "unique words = 737886\n",
      "################################\n",
      "\n",
      "\n",
      "The number of scipy stopwords is 179\n",
      "CPU times: user 5.88 s, sys: 28 ms, total: 5.91 s\n",
      "Wall time: 5.91 s\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0                                 sad apl friend ...      0\n",
      "1                        missed new moon trailer ...      0\n",
      "2                                 omg already 7:30 :      1\n",
      "3  .. omgaga . im sooo im gunna cry . i've dentis...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 737714\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 4.17 s, sys: 76 ms, total: 4.24 s\n",
      "Wall time: 4.24 s\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0                                 sad apl friend ...      0\n",
      "1                        missed new moon trailer ...      0\n",
      "2                                   omg already 7:30      1\n",
      "3  .. omgaga im sooo im gunna cry i've dentist si...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 737691\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 16.6 s, sys: 7.96 ms, total: 16.6 s\n",
      "Wall time: 16.6 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0                                 sad apl friend ...      0\n",
      "1                        missed new moon trailer ...      0\n",
      "2                                   omg already 7:30      1\n",
      "3  .. omgaga im sooo im gunna cry i've dentist si...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 737309\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 16.6 s, sys: 12 ms, total: 16.6 s\n",
      "Wall time: 16.6 s\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0                                 sad apl friend ...      0\n",
      "1                        missed new moon trailer ...      0\n",
      "2                                        omg already      1\n",
      "3  .. omgaga im sooo im gunna cry i've dentist si...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 729459\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 4.57 s, sys: 4.06 ms, total: 4.57 s\n",
      "Wall time: 4.57 s\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0                                 sad apl friend ...      0\n",
      "1                        missed new moon trailer ...      0\n",
      "2                                        omg already      1\n",
      "3  .. omgaga im sooo im gunna cry i've dentist si...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 720202\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 35.5 s, sys: 20 ms, total: 35.5 s\n",
      "Wall time: 35.5 s\n",
      "more_than_double_rep\n",
      "                                            sentence  label\n",
      "0                                  sad apl friend ..      0\n",
      "1                         missed new moon trailer ..      0\n",
      "2                                        omg already      1\n",
      "3  .. omgaga im soo im gunna cry i've dentist sin...      0\n",
      "4                     think mi bf cheating ! ! ! t_t      0\n",
      "unique words = 700510\n",
      "################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "for clean_option in cleaning_options:\n",
    "    %time df_cleaned = clean[clean_option](df_cleaned)\n",
    "    print(clean_option)\n",
    "    print(\"unique words = {}\".format(count_unique_words(df_cleaned)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.to_csv(\"Data/clean_extra_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
