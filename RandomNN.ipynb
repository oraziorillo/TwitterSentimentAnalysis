{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams, \\\n",
    "            build_unique_ngrams, create_sentence_vectors, create_sentence_vectors_submission\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import gensim   # Not sure whether it is better to use gensim or tensorflow :/\n",
    "import logging\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Embedding, Input, Conv1D, Dense, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataframes/full_df_cleaned_train_0_8.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"dataframes/full_df_cleaned_test_0_2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452521"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-17 20:49:31,992 : INFO : loading Word2Vec object from models/w2v_model_epochs_5_win_5_cbow_250.model\n",
      "2019-12-17 20:49:32,644 : INFO : loading wv recursively from models/w2v_model_epochs_5_win_5_cbow_250.model.wv.* with mmap=None\n",
      "2019-12-17 20:49:32,645 : INFO : loading vectors from models/w2v_model_epochs_5_win_5_cbow_250.model.wv.vectors.npy with mmap=None\n",
      "2019-12-17 20:49:33,476 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-12-17 20:49:33,476 : INFO : loading vocabulary recursively from models/w2v_model_epochs_5_win_5_cbow_250.model.vocabulary.* with mmap=None\n",
      "2019-12-17 20:49:33,477 : INFO : loading trainables recursively from models/w2v_model_epochs_5_win_5_cbow_250.model.trainables.* with mmap=None\n",
      "2019-12-17 20:49:33,477 : INFO : loading syn1neg from models/w2v_model_epochs_5_win_5_cbow_250.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-12-17 20:49:34,310 : INFO : setting ignored attribute cum_table to None\n",
      "2019-12-17 20:49:34,311 : INFO : loaded models/w2v_model_epochs_5_win_5_cbow_250.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"models/w2v_model_epochs_5_win_5_cbow_250.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.word_vec(\"love\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(w2v_model, word_index):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "\n",
    "    ## We can assume love is always present in our vocabulary ahaha\n",
    "    embedding_matrix = np.zeros((vocab_size, w2v_model.wv.word_vec(\"love\").shape[0]))  \n",
    "    \n",
    "    for word in w2v_model.wv.vocab:\n",
    "        vector = w2v_model.wv.word_vec(word)\n",
    "        if word in word_index:\n",
    "            idx = word_index[word] \n",
    "            embedding_matrix[idx] = np.array(\n",
    "                vector, dtype=np.float32)\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(df_test.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396986"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 1810, 8634, 2884, 3383, 7]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"want pottermore let ! can't obsessed let #nerdproblems\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8634].sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "def max_len(X):\n",
    "    maxlen = 0\n",
    "    for el in X:\n",
    "        maxlen = maxlen if len(el) < maxlen else len(el)\n",
    "    return maxlen\n",
    "maxlen = max_len(X_train)\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  15 1810 8634 2884 3383    7    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(\n",
    "    w2v_model,\n",
    "    tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d05b2437020c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnonzero_elements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnonzero_elements\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements / vocab_size)\n",
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert -1 in 0 (otherwise it doesn't work)\n",
    "y_train = np.where(df.label == 1, 1, 0)\n",
    "y_test = np.where(df_test.label == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 44, 250)           99246500  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 40, 50)            62550     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                3060      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 99,315,831\n",
      "Trainable params: 69,331\n",
      "Non-trainable params: 99,246,500\n",
      "_________________________________________________________________\n",
      "Train on 2000000 samples, validate on 500000 samples\n",
      "Epoch 1/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.4127 - acc: 0.8025\n",
      "Epoch 00001: val_acc improved from -inf to 0.80966, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 261s 130us/sample - loss: 0.4127 - acc: 0.8025 - val_loss: 0.4012 - val_acc: 0.8097\n",
      "Epoch 2/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3931 - acc: 0.8143\n",
      "Epoch 00002: val_acc improved from 0.80966 to 0.81350, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 306s 153us/sample - loss: 0.3931 - acc: 0.8143 - val_loss: 0.3940 - val_acc: 0.8135\n",
      "Epoch 3/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3853 - acc: 0.8186\n",
      "Epoch 00003: val_acc improved from 0.81350 to 0.81519, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 328s 164us/sample - loss: 0.3853 - acc: 0.8186 - val_loss: 0.3911 - val_acc: 0.8152\n",
      "Epoch 4/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3803 - acc: 0.8212\n",
      "Epoch 00004: val_acc improved from 0.81519 to 0.81547, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 338s 169us/sample - loss: 0.3803 - acc: 0.8212 - val_loss: 0.3899 - val_acc: 0.8155\n",
      "Epoch 5/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8234\n",
      "Epoch 00005: val_acc improved from 0.81547 to 0.81687, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 342s 171us/sample - loss: 0.3766 - acc: 0.8234 - val_loss: 0.3880 - val_acc: 0.8169\n",
      "Epoch 6/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3737 - acc: 0.8249\n",
      "Epoch 00006: val_acc did not improve from 0.81687\n",
      "2000000/2000000 [==============================] - 345s 173us/sample - loss: 0.3737 - acc: 0.8250 - val_loss: 0.3888 - val_acc: 0.8168\n",
      "Epoch 7/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3713 - acc: 0.8264\n",
      "Epoch 00007: val_acc improved from 0.81687 to 0.81691, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 356s 178us/sample - loss: 0.3713 - acc: 0.8264 - val_loss: 0.3889 - val_acc: 0.8169\n",
      "Epoch 8/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3692 - acc: 0.8276\n",
      "Epoch 00008: val_acc did not improve from 0.81691\n",
      "2000000/2000000 [==============================] - 365s 183us/sample - loss: 0.3692 - acc: 0.8276 - val_loss: 0.3897 - val_acc: 0.8164\n",
      "Epoch 9/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3675 - acc: 0.8284\n",
      "Epoch 00009: val_acc improved from 0.81691 to 0.81741, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 370s 185us/sample - loss: 0.3675 - acc: 0.8284 - val_loss: 0.3901 - val_acc: 0.8174\n",
      "Epoch 10/20\n",
      "1999488/2000000 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.8291\n",
      "Epoch 00010: val_acc did not improve from 0.81741\n",
      "2000000/2000000 [==============================] - 358s 179us/sample - loss: 0.3660 - acc: 0.8291 - val_loss: 0.3878 - val_acc: 0.8174\n",
      "Epoch 11/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3646 - acc: 0.8299\n",
      "Epoch 00011: val_acc improved from 0.81741 to 0.81824, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 379s 189us/sample - loss: 0.3646 - acc: 0.8299 - val_loss: 0.3887 - val_acc: 0.8182\n",
      "Epoch 12/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3634 - acc: 0.8307\n",
      "Epoch 00012: val_acc did not improve from 0.81824\n",
      "2000000/2000000 [==============================] - 339s 170us/sample - loss: 0.3634 - acc: 0.8307 - val_loss: 0.3880 - val_acc: 0.8177\n",
      "Epoch 13/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3623 - acc: 0.8313\n",
      "Epoch 00013: val_acc improved from 0.81824 to 0.81878, saving model to models/convolutional_nn_layers_of_size_50_60\n",
      "2000000/2000000 [==============================] - 365s 182us/sample - loss: 0.3623 - acc: 0.8312 - val_loss: 0.3878 - val_acc: 0.8188\n",
      "Epoch 14/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3613 - acc: 0.8316\n",
      "Epoch 00014: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 371s 186us/sample - loss: 0.3613 - acc: 0.8316 - val_loss: 0.3877 - val_acc: 0.8185\n",
      "Epoch 15/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3604 - acc: 0.8323\n",
      "Epoch 00015: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 375s 188us/sample - loss: 0.3604 - acc: 0.8323 - val_loss: 0.3890 - val_acc: 0.8181\n",
      "Epoch 16/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3595 - acc: 0.8328\n",
      "Epoch 00016: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 242s 121us/sample - loss: 0.3595 - acc: 0.8328 - val_loss: 0.3910 - val_acc: 0.8186\n",
      "Epoch 17/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3587 - acc: 0.8333\n",
      "Epoch 00017: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 230s 115us/sample - loss: 0.3587 - acc: 0.8333 - val_loss: 0.3902 - val_acc: 0.8178\n",
      "Epoch 18/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8335\n",
      "Epoch 00018: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 232s 116us/sample - loss: 0.3579 - acc: 0.8335 - val_loss: 0.3913 - val_acc: 0.8166\n",
      "Epoch 19/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3574 - acc: 0.8339\n",
      "Epoch 00019: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 231s 116us/sample - loss: 0.3574 - acc: 0.8339 - val_loss: 0.3923 - val_acc: 0.8182\n",
      "Epoch 20/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3566 - acc: 0.8345\n",
      "Epoch 00020: val_acc did not improve from 0.81878\n",
      "2000000/2000000 [==============================] - 232s 116us/sample - loss: 0.3566 - acc: 0.8345 - val_loss: 0.3932 - val_acc: 0.8180\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 44, 250)           99246500  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 40, 100)           125100    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 60)                3660      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 61        \n",
      "=================================================================\n",
      "Total params: 99,381,381\n",
      "Trainable params: 134,881\n",
      "Non-trainable params: 99,246,500\n",
      "_________________________________________________________________\n",
      "Train on 2000000 samples, validate on 500000 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.4085 - acc: 0.8052\n",
      "Epoch 00001: val_acc improved from -inf to 0.81334, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 318s 159us/sample - loss: 0.4085 - acc: 0.8052 - val_loss: 0.3944 - val_acc: 0.8133\n",
      "Epoch 2/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3867 - acc: 0.8179\n",
      "Epoch 00002: val_acc improved from 0.81334 to 0.81671, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 319s 159us/sample - loss: 0.3867 - acc: 0.8179 - val_loss: 0.3894 - val_acc: 0.8167\n",
      "Epoch 3/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3766 - acc: 0.8238\n",
      "Epoch 00003: val_acc improved from 0.81671 to 0.81941, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 319s 159us/sample - loss: 0.3766 - acc: 0.8238 - val_loss: 0.3842 - val_acc: 0.8194\n",
      "Epoch 4/20\n",
      "1999488/2000000 [============================>.] - ETA: 0s - loss: 0.3696 - acc: 0.8275\n",
      "Epoch 00004: val_acc did not improve from 0.81941\n",
      "2000000/2000000 [==============================] - 318s 159us/sample - loss: 0.3696 - acc: 0.8275 - val_loss: 0.3845 - val_acc: 0.8193\n",
      "Epoch 5/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3641 - acc: 0.8306\n",
      "Epoch 00005: val_acc did not improve from 0.81941\n",
      "2000000/2000000 [==============================] - 318s 159us/sample - loss: 0.3641 - acc: 0.8306 - val_loss: 0.3843 - val_acc: 0.8194\n",
      "Epoch 6/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8331\n",
      "Epoch 00006: val_acc improved from 0.81941 to 0.82084, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 322s 161us/sample - loss: 0.3597 - acc: 0.8331 - val_loss: 0.3839 - val_acc: 0.8208\n",
      "Epoch 7/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3561 - acc: 0.8350\n",
      "Epoch 00007: val_acc did not improve from 0.82084\n",
      "2000000/2000000 [==============================] - 319s 160us/sample - loss: 0.3561 - acc: 0.8350 - val_loss: 0.3849 - val_acc: 0.8197\n",
      "Epoch 8/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3530 - acc: 0.8369\n",
      "Epoch 00008: val_acc improved from 0.82084 to 0.82091, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 320s 160us/sample - loss: 0.3530 - acc: 0.8369 - val_loss: 0.3846 - val_acc: 0.8209\n",
      "Epoch 9/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3501 - acc: 0.8383\n",
      "Epoch 00009: val_acc did not improve from 0.82091\n",
      "2000000/2000000 [==============================] - 321s 160us/sample - loss: 0.3501 - acc: 0.8383 - val_loss: 0.3862 - val_acc: 0.8195\n",
      "Epoch 10/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3477 - acc: 0.8397\n",
      "Epoch 00010: val_acc improved from 0.82091 to 0.82117, saving model to models/convolutional_nn_layers_of_size_100_60\n",
      "2000000/2000000 [==============================] - 321s 161us/sample - loss: 0.3477 - acc: 0.8397 - val_loss: 0.3858 - val_acc: 0.8212\n",
      "Epoch 11/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3455 - acc: 0.8410\n",
      "Epoch 00011: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 321s 161us/sample - loss: 0.3455 - acc: 0.8411 - val_loss: 0.3843 - val_acc: 0.8208\n",
      "Epoch 12/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3435 - acc: 0.8419\n",
      "Epoch 00012: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 321s 160us/sample - loss: 0.3435 - acc: 0.8419 - val_loss: 0.3871 - val_acc: 0.8203\n",
      "Epoch 13/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3415 - acc: 0.8431\n",
      "Epoch 00013: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 320s 160us/sample - loss: 0.3415 - acc: 0.8431 - val_loss: 0.3881 - val_acc: 0.8201\n",
      "Epoch 14/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.8439\n",
      "Epoch 00014: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 321s 161us/sample - loss: 0.3399 - acc: 0.8439 - val_loss: 0.3889 - val_acc: 0.8205\n",
      "Epoch 15/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3382 - acc: 0.8450\n",
      "Epoch 00015: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 321s 161us/sample - loss: 0.3382 - acc: 0.8450 - val_loss: 0.3976 - val_acc: 0.8194\n",
      "Epoch 16/20\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.8456\n",
      "Epoch 00016: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 323s 161us/sample - loss: 0.3368 - acc: 0.8456 - val_loss: 0.3916 - val_acc: 0.8205\n",
      "Epoch 17/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.8463\n",
      "Epoch 00017: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 323s 161us/sample - loss: 0.3355 - acc: 0.8463 - val_loss: 0.3935 - val_acc: 0.8202\n",
      "Epoch 18/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.8472\n",
      "Epoch 00018: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 323s 161us/sample - loss: 0.3342 - acc: 0.8472 - val_loss: 0.3970 - val_acc: 0.8198\n",
      "Epoch 19/20\n",
      "1999744/2000000 [============================>.] - ETA: 0s - loss: 0.3329 - acc: 0.8480\n",
      "Epoch 00019: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 324s 162us/sample - loss: 0.3329 - acc: 0.8480 - val_loss: 0.3939 - val_acc: 0.8200\n",
      "Epoch 20/20\n",
      "1999616/2000000 [============================>.] - ETA: 0s - loss: 0.3319 - acc: 0.8484\n",
      "Epoch 00020: val_acc did not improve from 0.82117\n",
      "2000000/2000000 [==============================] - 323s 162us/sample - loss: 0.3319 - acc: 0.8484 - val_loss: 0.3930 - val_acc: 0.8181\n"
     ]
    }
   ],
   "source": [
    "for i in range(50, 251, 50):\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim,\n",
    "                               input_length=maxlen,\n",
    "                               weights=[embedding_matrix],\n",
    "                               trainable=False))\n",
    "    model.add(layers.Conv1D(i, 5, activation='relu'))   ## Maybe I should increase the kernel window (currently only 5)\n",
    "    model.add(layers.GlobalMaxPooling1D())\n",
    "    model.add(layers.Dense(60, activation='relu'))\n",
    "    model.add(layers.Dense(60, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    filepath=\"models/convolutional_nn_layers_of_size_{}_{}\".format(i, 60)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=20,\n",
    "                        verbose=True,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000000 samples, validate on 500000 samples\n",
      "Epoch 1/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.4058 - acc: 0.8064\n",
      "Epoch 00001: val_acc improved from -inf to 0.81485, saving model to models/convolutional_nn_layers_of_size_256_30\n",
      "2000000/2000000 [==============================] - 1566s 783us/sample - loss: 0.4058 - acc: 0.8064 - val_loss: 0.3916 - val_acc: 0.8148\n",
      "Epoch 2/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3780 - acc: 0.8225\n",
      "Epoch 00002: val_acc improved from 0.81485 to 0.81871, saving model to models/convolutional_nn_layers_of_size_256_30\n",
      "2000000/2000000 [==============================] - 1586s 793us/sample - loss: 0.3780 - acc: 0.8225 - val_loss: 0.3842 - val_acc: 0.8187\n",
      "Epoch 3/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3597 - acc: 0.8328\n",
      "Epoch 00003: val_acc improved from 0.81871 to 0.81954, saving model to models/convolutional_nn_layers_of_size_256_30\n",
      "2000000/2000000 [==============================] - 1629s 814us/sample - loss: 0.3597 - acc: 0.8328 - val_loss: 0.3835 - val_acc: 0.8195\n",
      "Epoch 4/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.8416\n",
      "Epoch 00004: val_acc did not improve from 0.81954\n",
      "2000000/2000000 [==============================] - 1662s 831us/sample - loss: 0.3432 - acc: 0.8416 - val_loss: 0.3895 - val_acc: 0.8174\n",
      "Epoch 5/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3286 - acc: 0.8496\n",
      "Epoch 00005: val_acc did not improve from 0.81954\n",
      "2000000/2000000 [==============================] - 1664s 832us/sample - loss: 0.3286 - acc: 0.8496 - val_loss: 0.3938 - val_acc: 0.8189\n",
      "Epoch 6/10\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3157 - acc: 0.8565\n",
      "Epoch 00006: val_acc did not improve from 0.81954\n",
      "2000000/2000000 [==============================] - 1665s 833us/sample - loss: 0.3157 - acc: 0.8565 - val_loss: 0.4155 - val_acc: 0.8182\n",
      "Epoch 7/10\n",
      " 576768/2000000 [=======>......................] - ETA: 18:50 - loss: 0.2972 - acc: 0.8663"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d28ddee54cc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                     batch_size=128)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/machine_learning/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filepath=\"models/convolutional_nn_layers_of_size_{}_{}\".format(256, 30)\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "### don't add this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 44, 200)           79397200  \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                6030      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 79,403,261\n",
      "Trainable params: 79,403,261\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000000 samples, validate on 500000 samples\n",
      "Epoch 1/30\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3982 - acc: 0.8114\n",
      "Epoch 00001: val_acc improved from -inf to 0.82468, saving model to models/random_nn_layers_of_size_30_big\n",
      "2000000/2000000 [==============================] - 1074s 537us/sample - loss: 0.3982 - acc: 0.8114 - val_loss: 0.3758 - val_acc: 0.8247\n",
      "Epoch 2/30\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.8432\n",
      "Epoch 00002: val_acc improved from 0.82468 to 0.82823, saving model to models/random_nn_layers_of_size_30_big\n",
      "2000000/2000000 [==============================] - 1078s 539us/sample - loss: 0.3439 - acc: 0.8432 - val_loss: 0.3695 - val_acc: 0.8282\n",
      "Epoch 3/30\n",
      "1999872/2000000 [============================>.] - ETA: 0s - loss: 0.3073 - acc: 0.8632\n",
      "Epoch 00003: val_acc did not improve from 0.82823\n",
      "2000000/2000000 [==============================] - 1074s 537us/sample - loss: 0.3073 - acc: 0.8632 - val_loss: 0.3769 - val_acc: 0.8282\n",
      "Epoch 4/30\n",
      " 688640/2000000 [=========>....................] - ETA: 11:42 - loss: 0.2711 - acc: 0.8815"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7947b0a29474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                         batch_size=128)\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Accuracy: {:.4f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(30, 101, 10):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                               weights=[embedding_matrix], \n",
    "                               input_length=maxlen, \n",
    "                               trainable=False))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dense(i, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    filepath=\"models/random_nn_layers_of_size_{}_big\".format(i)\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=30,\n",
    "                        verbose=True,\n",
    "                        validation_data=(X_test, y_test),\n",
    "                        callbacks=callbacks_list,\n",
    "                        batch_size=128)\n",
    "    loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "    print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "    print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "    print(\"#############################\\n############################\\n##################################\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
