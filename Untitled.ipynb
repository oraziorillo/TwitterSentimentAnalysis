{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams, \\\n",
    "            build_unique_ngrams, create_sentence_vectors, create_sentence_vectors_submission\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import gensim   # Not sure whether it is better to use gensim or tensorflow :/\n",
    "import logging\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"dataframes/full_df_cleaned_train_0_8.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_pickle(\"dataframes/full_df_cleaned_test_0_2.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452521"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = tokenizer.texts_to_sequences(df_test.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396986"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 1810, 8634, 2884, 3383, 7]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[X_train[0] == '#']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    want pottermore let ! can't obsessed let #nerd...\n",
       "label                                                      -1\n",
       "Name: 2109505, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[8634]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_len(X):\n",
    "    maxlen = 0\n",
    "    for el in X:\n",
    "        maxlen = maxlen if len(el) < maxlen else len(el)\n",
    "    return maxlen\n",
    "maxlen = max_len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  15 1810 8634 2884 3383    7    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = create_embedding_matrix(\n",
    "    'models/glove.6B/glove.6B.50d.txt',\n",
    "    tokenizer.word_index, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396986"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "nonzero_elements / vocab_size\n",
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 44, 50)            19849300  \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                510       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 19,849,821\n",
      "Trainable params: 19,849,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, \n",
    "                           weights=[embedding_matrix], \n",
    "                           input_length=maxlen, \n",
    "                           trainable=True))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where(df.label == 1, 1, 0)\n",
    "y_test = np.where(df_test.label == 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000000 samples, validate on 500000 samples\n",
      "Epoch 1/50\n",
      "2000000/2000000 [==============================] - 926s 463us/sample - loss: 0.4829 - accuracy: 0.7589 - val_loss: 0.4300 - val_accuracy: 0.7950\n",
      "Epoch 2/50\n",
      "2000000/2000000 [==============================] - 932s 466us/sample - loss: 0.4070 - accuracy: 0.8089 - val_loss: 0.4077 - val_accuracy: 0.8075\n",
      "Epoch 3/50\n",
      "2000000/2000000 [==============================] - 1372s 686us/sample - loss: 0.3818 - accuracy: 0.8238 - val_loss: 0.4005 - val_accuracy: 0.8120\n",
      "Epoch 4/50\n",
      "  97792/2000000 [>.............................] - ETA: 23:37 - loss: 0.3642 - accuracy: 0.8348"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=50,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=512)\n",
    "loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "plot_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning] *",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
