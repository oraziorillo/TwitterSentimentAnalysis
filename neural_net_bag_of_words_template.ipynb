{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = False\n",
    "test_locally = True\n",
    "create_new_text_files = True\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_options = ['clean_new_line', 'remove_stopwords', 'clean_tags',\n",
    "                    'clean_punctuation', 'remove_numbers', 'lemmatize', 'remove_saxon_genitive',\n",
    "                    ]\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive,\n",
    "    \"gensim_simple\": gensim_clean   # not a good idea to use it I think! It cleans everything which is not alphabetic (special char, numbers and so on)\n",
    "}\n",
    "\n",
    "\n",
    "# algorithm_used = \"\"\n",
    "# algorithm = {\n",
    "#     \"naive_bayes\": ,\n",
    "#     \"logistic_regression\": ,\n",
    "#     \"svm\": ,\n",
    "#     \"lstm\":,\n",
    "#     \"fasttext\":,\n",
    "#     \"cnn\": ,\n",
    "# }\n",
    "\n",
    "# options = []\n",
    "# additional_options = {\n",
    "#     \"count_frequency\": ,\n",
    "#     \"count_ngrams\": ,\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_pos = 'Data/train_pos.txt'\n",
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "  \n",
    "input_file_neg = 'Data/train_neg.txt'\n",
    "if take_full:\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "    \n",
    "list_of_pos_sentences = []\n",
    "with open(input_file_pos, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_pos_sentences.append(line)\n",
    " \n",
    "list_of_neg_sentences = []\n",
    "with open(input_file_neg, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_neg_sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 114427\n"
     ]
    }
   ],
   "source": [
    "from data_handling import build_sentences\n",
    "\n",
    "df = build_sentences(list_of_pos_sentences, list_of_neg_sentences)\n",
    "\n",
    "print(\"unique words = {}\".format(count_unique_words(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 262 ms, sys: 6 Âµs, total: 262 ms\n",
      "Wall time: 261 ms\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2  \" <user> just put casper in a box ! \" looved t...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "unique words = 114427\n",
      "################################\n",
      "\n",
      "\n",
      "The number of scipy stopwords is 179\n",
      "CPU times: user 597 ms, sys: 0 ns, total: 597 ms\n",
      "Wall time: 597 ms\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  <user> dunno justin read mention . justin god ...      1\n",
      "1    logic dumb , even crop name photo . tsk . <url>      1\n",
      "2  \" <user> put casper box ! \" looved battle ! #c...      1\n",
      "3  <user> <user> thanks sir > > trip lil mama ......      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114257\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1.88 s, sys: 3.86 ms, total: 1.88 s\n",
      "Wall time: 1.88 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention . justin god knows ,...      1\n",
      "1          logic dumb , even crop name photo . tsk .      1\n",
      "2   \" put casper box ! \" looved battle ! #crakkbitch      1\n",
      "3  thanks sir > > trip lil mama ... keep doin ya ...      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114232\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 437 ms, sys: 0 ns, total: 437 ms\n",
      "Wall time: 437 ms\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114215\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1.66 s, sys: 0 ns, total: 1.66 s\n",
      "Wall time: 1.66 s\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 108252\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 6.5 s, sys: 0 ns, total: 6.5 s\n",
      "Wall time: 6.5 s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 102625\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 425 ms, sys: 0 ns, total: 425 ms\n",
      "Wall time: 425 ms\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 100500\n",
      "################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dunno justin read mention justin god know hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logic dumb even crop name photo tsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>put casper box ! looved battle ! #crakkbitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks sir trip lil mama ... keep doin ya thang !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting brother tmr bestest birthday gift eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  dunno justin read mention justin god know hope...      1\n",
       "1                logic dumb even crop name photo tsk      1\n",
       "2       put casper box ! looved battle ! #crakkbitch      1\n",
       "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
       "4  visiting brother tmr bestest birthday gift eve...      1"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "\n",
    "for clean_option in cleaning_options:\n",
    "    counter_of_occurrences = 0\n",
    "    %time df = clean[clean_option](df)\n",
    "    print(clean_option)\n",
    "    print(df.head())\n",
    "    print(\"unique words = {}\".format(count_unique_words(df)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>83074</td>\n",
       "      <td>83074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...</td>\n",
       "      <td>40967</td>\n",
       "      <td>40967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>26418</td>\n",
       "      <td>26418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i'm</td>\n",
       "      <td>13656</td>\n",
       "      <td>13656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>..</td>\n",
       "      <td>9456</td>\n",
       "      <td>9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>can't</td>\n",
       "      <td>4905</td>\n",
       "      <td>4905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>*</td>\n",
       "      <td>4519</td>\n",
       "      <td>4519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>4361</td>\n",
       "      <td>4361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>i'll</td>\n",
       "      <td>3205</td>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>:d</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>i've</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>#</td>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>|</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>:p</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>^</td>\n",
       "      <td>1032</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>$</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>+</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>~</td>\n",
       "      <td>854</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>:/</td>\n",
       "      <td>825</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>i'd</td>\n",
       "      <td>825</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  index  label\n",
       "0        !  83074  83074\n",
       "1      ...  40967  40967\n",
       "2        ?  26418  26418\n",
       "3      i'm  13656  13656\n",
       "8       ..   9456   9456\n",
       "28   can't   4905   4905\n",
       "32       *   4519   4519\n",
       "36      <3   4361   4361\n",
       "50    i'll   3205   3205\n",
       "113     :d   1939   1939\n",
       "118   i've   1851   1851\n",
       "188      #   1150   1150\n",
       "204      |   1090   1090\n",
       "214     :p   1058   1058\n",
       "227      ^   1032   1032\n",
       "277      $    877    877\n",
       "281      +    864    864\n",
       "285      ~    854    854\n",
       "295     :/    825    825\n",
       "296    i'd    825    825"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "df_copy['word'] = df_copy.sentence.apply(lambda x: x.split((\" \")))\n",
    "\n",
    "df_copy = df_copy.drop(\"sentence\", axis=1)\n",
    "\n",
    "df_exploded = df_copy.explode(\"word\").reindex()\n",
    "\n",
    "df_exploded = df_exploded.reset_index()\n",
    "\n",
    "df_grouped = df_exploded.groupby(\"word\").count().sort_values(by='index', ascending=False).reset_index()\n",
    "\n",
    "df_non_alpha = df_grouped[df_grouped['word'].apply(lambda x: not x.isalpha())]\n",
    "\n",
    "df_non_alpha.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100500"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100500"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sentence'].apply(lambda x: x.split()).explode().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1010245"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_ngrams(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_labelled_file\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.27 s, sys: 0 ns, total: 1.27 s\n",
      "Wall time: 1.28 s\n",
      "(200000, 100500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if test_locally:    \n",
    "    # Create the bag of words\n",
    "    # The token_pattern is used in order to avoid the preprocessor to remove special characters (like smiles)\n",
    "    # or hashtags (Twitter!)\n",
    "    vectorizer = CountVectorizer(vocabulary=df['sentence'].apply(lambda x: x.split()).explode().unique())  # The vectorizer is used to create the bag of words\n",
    "\n",
    "    X = vectorizer.fit_transform(df['sentence'])\n",
    "    Y = df['label']\n",
    "    \n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 102408)\n",
      "9\n",
      "dunno justin read mention justin god know hope follow #believe\n",
      "2723\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(X.shape)\n",
    "for x in X[0].toarray()[0]:\n",
    "    if x > 0:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(df.iloc[0].sentence)\n",
    "print(vectorizer.vocabulary_.get('#crakkbitch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750000, 403138)\n",
      "(750000, 403138)\n",
      "2253463    0\n",
      "1388299    0\n",
      "208807     1\n",
      "1205350    1\n",
      "1607799    0\n",
      "          ..\n",
      "1462319    0\n",
      "1126134    1\n",
      "331413     1\n",
      "1876117    0\n",
      "1804629    0\n",
      "Name: label, Length: 1750000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# divide in train test split\n",
    "# CAREFUL HERE: when working with neural nets, we need to convert -1, 1 labels into 0, 1\n",
    "if test_locally:\n",
    "    train_test_split = 0.7\n",
    "    permut = np.random.permutation(X.shape[0])\n",
    "    train_x = X[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    train_y = Y[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    \n",
    "    test_x = X[permut[int(X.shape[0]*train_test_split):]]\n",
    "    test_y = Y[permut[int(X.shape[0]*train_test_split):]]\n",
    "    \n",
    "    ## Convert all -1 into 0!\n",
    "    train_y = train_y.where(train_y == 1, 0) \n",
    "    test_y = test_y.where(test_y == 1, 0)\n",
    "    \n",
    "    print(train_x.shape)\n",
    "    print(test_x.shape)\n",
    "    print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "    keras.layers.Dense(40, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750000/1750000 [==============================] - 3069s 2ms/sample - loss: 0.4354 - acc: 0.7983\n",
      " - 18s - loss: 0.4039 - acc: 0.8113\n",
      "1750000/1750000 [==============================] - 3060s 2ms/sample - loss: 0.3783 - acc: 0.8249\n",
      " - 18s - loss: 0.3974 - acc: 0.8171\n",
      "1750000/1750000 [==============================] - 3060s 2ms/sample - loss: 0.3455 - acc: 0.8431\n",
      " - 18s - loss: 0.3992 - acc: 0.8194\n",
      "1750000/1750000 [==============================] - 3060s 2ms/sample - loss: 0.3156 - acc: 0.8584\n",
      " - 18s - loss: 0.4042 - acc: 0.8220\n",
      "1750000/1750000 [==============================] - 3064s 2ms/sample - loss: 0.2885 - acc: 0.8722\n",
      " - 18s - loss: 0.4174 - acc: 0.8210\n",
      "1750000/1750000 [==============================] - 3063s 2ms/sample - loss: 0.2651 - acc: 0.8838\n",
      " - 18s - loss: 0.4376 - acc: 0.8188\n",
      "1750000/1750000 [==============================] - 3068s 2ms/sample - loss: 0.2455 - acc: 0.8935\n",
      " - 18s - loss: 0.4585 - acc: 0.8159\n",
      "1750000/1750000 [==============================] - 3072s 2ms/sample - loss: 0.2292 - acc: 0.9013\n",
      " - 18s - loss: 0.4830 - acc: 0.8123\n"
     ]
    }
   ],
   "source": [
    "for iter in range(8):\n",
    "    # train for 5 epochs the model \n",
    "    model.fit(train_x, train_y, epochs=1, batch_size=2048)\n",
    "    # evaluate the test error\n",
    "    model.evaluate(test_x[:10000],  test_y[:10000], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750000/1750000 [==============================] - 3194s 2ms/sample - loss: 0.4105 - acc: 0.8075\n",
      " - 1420s - loss: 0.4153 - acc: 0.8047\n",
      "1750000/1750000 [==============================] - 3295s 2ms/sample - loss: 0.4092 - acc: 0.8082\n",
      " - 1385s - loss: 0.4143 - acc: 0.8052\n",
      "1750000/1750000 [==============================] - 3415s 2ms/sample - loss: 0.4080 - acc: 0.8089\n",
      " - 1371s - loss: 0.4133 - acc: 0.8056\n",
      "1750000/1750000 [==============================] - 3132s 2ms/sample - loss: 0.4068 - acc: 0.8095\n",
      " - 1388s - loss: 0.4125 - acc: 0.8059\n",
      "1750000/1750000 [==============================] - 3118s 2ms/sample - loss: 0.4057 - acc: 0.8101\n",
      " - 1331s - loss: 0.4117 - acc: 0.8064\n",
      "1750000/1750000 [==============================] - 3092s 2ms/sample - loss: 0.4046 - acc: 0.8107\n",
      " - 1337s - loss: 0.4110 - acc: 0.8071\n",
      " 881664/1750000 [==============>...............] - ETA: 27:08 - loss: 0.4031 - acc: 0.8116"
     ]
    }
   ],
   "source": [
    "# done for 16 epochs\n",
    "for iter in range(8):\n",
    "    # train for 5 epochs the model \n",
    "    model.fit(train_x, train_y, epochs=1, batch_size=512)\n",
    "    # evaluate the test error\n",
    "    model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_big_16_epochs.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.4323 - acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_locally:\n",
    "    # Do cross validation on the bag of words, using the neural network\n",
    "    df_precisions = {}\n",
    "    for epochs in tqdm(range(10, 20, 2)):\n",
    "        precisions = []\n",
    "        for k in range(k_folds):\n",
    "            \n",
    "        df_precisions[epochs] = precisions\n",
    "        print(np.array(precisions).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
