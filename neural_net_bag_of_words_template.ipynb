{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = True\n",
    "test_locally = True\n",
    "create_new_text_files = True\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_options = ['clean_new_line', 'remove_stopwords', 'clean_tags',\n",
    "                    'clean_punctuation', 'remove_numbers', 'lemmatize', 'remove_saxon_genitive',\n",
    "                   'gensim_simple']\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive,\n",
    "    \"gensim_simple\": gensim_clean\n",
    "}\n",
    "\n",
    "\n",
    "# algorithm_used = \"\"\n",
    "# algorithm = {\n",
    "#     \"naive_bayes\": ,\n",
    "#     \"logistic_regression\": ,\n",
    "#     \"svm\": ,\n",
    "#     \"lstm\":,\n",
    "#     \"fasttext\":,\n",
    "#     \"cnn\": ,\n",
    "# }\n",
    "\n",
    "# options = []\n",
    "# additional_options = {\n",
    "#     \"count_frequency\": ,\n",
    "#     \"count_ngrams\": ,\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_pos = 'Data/train_pos.txt'\n",
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "  \n",
    "input_file_neg = 'Data/train_neg.txt'\n",
    "if take_full:\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "    \n",
    "list_of_pos_sentences = []\n",
    "with open(input_file_pos, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_pos_sentences.append(line)\n",
    " \n",
    "list_of_neg_sentences = []\n",
    "with open(input_file_neg, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_neg_sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 652183\n"
     ]
    }
   ],
   "source": [
    "from data_handling import build_sentences\n",
    "\n",
    "df = build_sentences(list_of_pos_sentences, list_of_neg_sentences)\n",
    "\n",
    "print(\"unique words = {}\".format(count_unique_words(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.23 s, sys: 112 ms, total: 4.35 s\n",
      "Wall time: 4.34 s\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2  \" <user> just put casper in a box ! \" looved t...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "unique words = 592563\n",
      "################################\n",
      "\n",
      "\n",
      "179\n",
      "CPU times: user 9.38 s, sys: 184 ms, total: 9.57 s\n",
      "Wall time: 9.57 s\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  <user> dunno justin read mention . justin god ...      1\n",
      "1    logic dumb , even crop name photo . tsk . <url>      1\n",
      "2  \" <user> put casper box ! \" looved battle ! #c...      1\n",
      "3  <user> <user> thanks sir > > trip lil mama ......      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592388\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 29.7 s, sys: 275 ms, total: 30 s\n",
      "Wall time: 30 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention . justin god knows ,...      1\n",
      "1          logic dumb , even crop name photo . tsk .      1\n",
      "2   \" put casper box ! \" looved battle ! #crakkbitch      1\n",
      "3  thanks sir > > trip lil mama ... keep doin ya ...      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592239\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 6.6 s, sys: 0 ns, total: 6.6 s\n",
      "Wall time: 6.6 s\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592223\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 10.4 s, sys: 7.3 ms, total: 10.4 s\n",
      "Wall time: 10.4 s\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 573315\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1min 51s, sys: 40.3 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 559207\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 6.9 s, sys: 0 ns, total: 6.9 s\n",
      "Wall time: 6.91 s\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 546805\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 26.2 s, sys: 10.3 ms, total: 26.2 s\n",
      "Wall time: 26.2 s\n",
      "gensim_simple\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2            put casper box looved battle crakkbitch      1\n",
      "3        thanks sir trip lil mama keep doin ya thang      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 404372\n",
      "################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dunno justin read mention justin god know hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>logic dumb even crop name photo tsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>put casper box looved battle crakkbitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>thanks sir trip lil mama keep doin ya thang</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>visiting brother tmr bestest birthday gift eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  dunno justin read mention justin god know hope...      1\n",
       "1                logic dumb even crop name photo tsk      1\n",
       "2            put casper box looved battle crakkbitch      1\n",
       "3        thanks sir trip lil mama keep doin ya thang      1\n",
       "4  visiting brother tmr bestest birthday gift eve...      1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "\n",
    "for clean_option in cleaning_options:\n",
    "    counter_of_occurrences = 0\n",
    "    %time df = clean[clean_option](df)\n",
    "    print(clean_option)\n",
    "    print(df.head())\n",
    "    print(\"unique words = {}\".format(count_unique_words(df)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>439</td>\n",
       "      <td></td>\n",
       "      <td>532</td>\n",
       "      <td>532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3132</td>\n",
       "      <td>o_o</td>\n",
       "      <td>57</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4658</td>\n",
       "      <td>x_x</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10407</td>\n",
       "      <td>t_t</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12405</td>\n",
       "      <td>t___t</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12658</td>\n",
       "      <td>ar_</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16099</td>\n",
       "      <td>indo_elfs</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17101</td>\n",
       "      <td>l_</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20563</td>\n",
       "      <td>jenny_meszaros</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22742</td>\n",
       "      <td>u_u</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22821</td>\n",
       "      <td>ukiss_amazing</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23386</td>\n",
       "      <td>o_</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23602</td>\n",
       "      <td>jayson_scott</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24389</td>\n",
       "      <td>s_h_o_u_t_o_u_t</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24586</td>\n",
       "      <td>n_n</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25480</td>\n",
       "      <td>tt_tt</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25690</td>\n",
       "      <td>lekkere_herrie</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25930</td>\n",
       "      <td>halo_lounge</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26998</td>\n",
       "      <td>m_igrill</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27261</td>\n",
       "      <td>sjfor_elfindo</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  index  label\n",
       "439                       532    532\n",
       "3132               o_o     57     57\n",
       "4658               x_x     33     33\n",
       "10407              t_t     10     10\n",
       "12405            t___t      8      8\n",
       "12658              ar_      7      7\n",
       "16099        indo_elfs      5      5\n",
       "17101               l_      5      5\n",
       "20563   jenny_meszaros      4      4\n",
       "22742              u_u      3      3\n",
       "22821    ukiss_amazing      3      3\n",
       "23386               o_      3      3\n",
       "23602     jayson_scott      3      3\n",
       "24389  s_h_o_u_t_o_u_t      3      3\n",
       "24586              n_n      3      3\n",
       "25480            tt_tt      3      3\n",
       "25690   lekkere_herrie      2      2\n",
       "25930      halo_lounge      2      2\n",
       "26998         m_igrill      2      2\n",
       "27261    sjfor_elfindo      2      2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "df_copy['word'] = df_copy.sentence.apply(lambda x: x.split((\" \")))\n",
    "\n",
    "df_copy = df_copy.drop(\"sentence\", axis=1)\n",
    "\n",
    "df_exploded = df_copy.explode(\"word\").reindex()\n",
    "\n",
    "df_exploded = df_exploded.reset_index()\n",
    "\n",
    "df_grouped = df_exploded.groupby(\"word\").count().sort_values(by='index', ascending=False).reset_index()\n",
    "\n",
    "df_non_alpha = df_grouped[df_grouped['word'].apply(lambda x: not x.isalpha())]\n",
    "\n",
    "df_non_alpha.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404372"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "864983"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_ngrams(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_labelled_file\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 264 ms, total: 19.4 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if test_locally:    \n",
    "    # Create the bag of words\n",
    "    # The token_pattern is used in order to avoid the preprocessor to remove special characters (like smiles)\n",
    "    # or hashtags (Twitter!)\n",
    "    vectorizer = CountVectorizer(token_pattern = '[a-zA-Z0-9$&+,:;=?@#|<>.^*()%!-]+')  # The vectorizer is used to create the bag of words\n",
    "\n",
    "    %time X = vectorizer.fit_transform(df['sentence'])\n",
    "    Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 102408)\n",
      "9\n",
      "dunno justin read mention justin god know hope follow #believe\n",
      "2723\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(X.shape)\n",
    "for x in X[0].toarray()[0]:\n",
    "    if x > 0:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(df.iloc[0].sentence)\n",
    "print(vectorizer.vocabulary_.get('#crakkbitch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1250000, 403138)\n",
      "(1250000, 403138)\n",
      "704958     1\n",
      "1600678    0\n",
      "1662348    0\n",
      "2128732    0\n",
      "600991     1\n",
      "          ..\n",
      "1220433    1\n",
      "359386     1\n",
      "2304515    0\n",
      "24279      1\n",
      "1394550    0\n",
      "Name: label, Length: 1250000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# divide in train test split\n",
    "# CAREFUL HERE: when working with neural nets, we need to convert -1, 1 labels into 0, 1\n",
    "if test_locally:\n",
    "    train_test_split = 0.5\n",
    "    permut = np.random.permutation(X.shape[0])\n",
    "    train_x = X[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    train_y = Y[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    \n",
    "    test_x = X[permut[int(X.shape[0]*train_test_split):]]\n",
    "    test_y = Y[permut[int(X.shape[0]*train_test_split):]]\n",
    "    \n",
    "    ## Convert all -1 into 0!\n",
    "    train_y = train_y.where(train_y == 1, 0) \n",
    "    test_y = test_y.where(test_y == 1, 0)\n",
    "    \n",
    "    print(train_x.shape)\n",
    "    print(test_x.shape)\n",
    "    print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 912384/1250000 [====================>.........] - ETA: 36:51 - loss: 0.6428 - acc: 0.6961"
     ]
    }
   ],
   "source": [
    "for iter in range(8):\n",
    "    # train for 5 epochs the model \n",
    "    model.fit(train_x, train_y, epochs=1, batch_size=512)\n",
    "    # evaluate the test error\n",
    "    model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.4323 - acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_locally:\n",
    "    # Do cross validation on the bag of words, using the neural network\n",
    "    df_precisions = {}\n",
    "    for epochs in tqdm(range(10, 20, 2)):\n",
    "        precisions = []\n",
    "        for k in range(k_folds):\n",
    "            \n",
    "        df_precisions[epochs] = precisions\n",
    "        print(np.array(precisions).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_learning] *",
   "language": "python",
   "name": "conda-env-machine_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
