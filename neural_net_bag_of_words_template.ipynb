{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = False\n",
    "test_locally = True\n",
    "create_new_text_files = True\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_options = ['clean_new_line', 'remove_stopwords', 'clean_tags',\n",
    "                    'clean_punctuation', 'remove_numbers', 'lemmatize', 'remove_saxon_genitive']\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive\n",
    "}\n",
    "\n",
    "\n",
    "# algorithm_used = \"\"\n",
    "# algorithm = {\n",
    "#     \"naive_bayes\": ,\n",
    "#     \"logistic_regression\": ,\n",
    "#     \"svm\": ,\n",
    "#     \"lstm\":,\n",
    "#     \"fasttext\":,\n",
    "#     \"cnn\": ,\n",
    "# }\n",
    "\n",
    "# options = []\n",
    "# additional_options = {\n",
    "#     \"count_frequency\": ,\n",
    "#     \"count_ngrams\": ,\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_pos = 'Data/train_pos.txt'\n",
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "  \n",
    "input_file_neg = 'Data/train_neg.txt'\n",
    "if take_full:\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "    \n",
    "list_of_pos_sentences = []\n",
    "with open(input_file_pos, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_pos_sentences.append(line)\n",
    " \n",
    "list_of_neg_sentences = []\n",
    "with open(input_file_neg, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_neg_sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 125642\n"
     ]
    }
   ],
   "source": [
    "from data_handling import build_sentences\n",
    "\n",
    "df = build_sentences(list_of_pos_sentences, list_of_neg_sentences)\n",
    "\n",
    "print(\"unique words = {}\".format(count_unique_words(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 252 ms, sys: 7.99 ms, total: 260 ms\n",
      "Wall time: 260 ms\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2  \" <user> just put casper in a box ! \" looved t...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "unique words = 114427\n",
      "################################\n",
      "\n",
      "\n",
      "179\n",
      "CPU times: user 591 ms, sys: 15.9 ms, total: 606 ms\n",
      "Wall time: 606 ms\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  <user> dunno justin read mention . justin god ...      1\n",
      "1    logic dumb , even crop name photo . tsk . <url>      1\n",
      "2  \" <user> put casper box ! \" looved battle ! #c...      1\n",
      "3  <user> <user> thanks sir > > trip lil mama ......      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114257\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1.79 s, sys: 7.67 ms, total: 1.8 s\n",
      "Wall time: 1.8 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention . justin god knows ,...      1\n",
      "1          logic dumb , even crop name photo . tsk .      1\n",
      "2   \" put casper box ! \" looved battle ! #crakkbitch      1\n",
      "3  thanks sir > > trip lil mama ... keep doin ya ...      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114232\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 409 ms, sys: 3.99 ms, total: 413 ms\n",
      "Wall time: 413 ms\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114216\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 633 ms, sys: 0 ns, total: 633 ms\n",
      "Wall time: 632 ms\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 110923\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 6.19 s, sys: 0 ns, total: 6.19 s\n",
      "Wall time: 6.19 s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 105295\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 514 ms, sys: 0 ns, total: 514 ms\n",
      "Wall time: 514 ms\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 103170\n",
      "################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dunno justin read mention justin god know hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logic dumb even crop name photo tsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>put casper box ! looved battle ! #crakkbitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks sir trip lil mama .. keep doin ya thang !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting brother tmr bestest birthday gift eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  dunno justin read mention justin god know hope...      1\n",
       "1                logic dumb even crop name photo tsk      1\n",
       "2       put casper box ! looved battle ! #crakkbitch      1\n",
       "3   thanks sir trip lil mama .. keep doin ya thang !      1\n",
       "4  visiting brother tmr bestest birthday gift eve...      1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "\n",
    "for clean_option in cleaning_options:\n",
    "    counter_of_occurrences = 0\n",
    "    %time df = clean[clean_option](df)\n",
    "    print(clean_option)\n",
    "    print(df.head())\n",
    "    print(\"unique words = {}\".format(count_unique_words(df)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>index</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!</td>\n",
       "      <td>83074</td>\n",
       "      <td>83074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..</td>\n",
       "      <td>40967</td>\n",
       "      <td>40967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>?</td>\n",
       "      <td>26418</td>\n",
       "      <td>26418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i'm</td>\n",
       "      <td>13656</td>\n",
       "      <td>13656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>9456</td>\n",
       "      <td>9456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&amp;</td>\n",
       "      <td>7799</td>\n",
       "      <td>7799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>can't</td>\n",
       "      <td>4905</td>\n",
       "      <td>4905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>*</td>\n",
       "      <td>4519</td>\n",
       "      <td>4519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;3</td>\n",
       "      <td>4361</td>\n",
       "      <td>4361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>i'll</td>\n",
       "      <td>3205</td>\n",
       "      <td>3205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>:d</td>\n",
       "      <td>1939</td>\n",
       "      <td>1939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>i've</td>\n",
       "      <td>1851</td>\n",
       "      <td>1851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>#</td>\n",
       "      <td>1150</td>\n",
       "      <td>1150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>|</td>\n",
       "      <td>1090</td>\n",
       "      <td>1090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>:p</td>\n",
       "      <td>1058</td>\n",
       "      <td>1058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>^</td>\n",
       "      <td>1032</td>\n",
       "      <td>1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>$</td>\n",
       "      <td>877</td>\n",
       "      <td>877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>+</td>\n",
       "      <td>864</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>~</td>\n",
       "      <td>854</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>:/</td>\n",
       "      <td>825</td>\n",
       "      <td>825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      word  index  label\n",
       "0        !  83074  83074\n",
       "1       ..  40967  40967\n",
       "2        ?  26418  26418\n",
       "3      i'm  13656  13656\n",
       "8        .   9456   9456\n",
       "13       &   7799   7799\n",
       "29   can't   4905   4905\n",
       "33       *   4519   4519\n",
       "37      <3   4361   4361\n",
       "51    i'll   3205   3205\n",
       "114     :d   1939   1939\n",
       "119   i've   1851   1851\n",
       "189      #   1150   1150\n",
       "205      |   1090   1090\n",
       "215     :p   1058   1058\n",
       "228      ^   1032   1032\n",
       "278      $    877    877\n",
       "282      +    864    864\n",
       "286      ~    854    854\n",
       "296     :/    825    825"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "df_copy['word'] = df_copy.sentence.apply(lambda x: x.split((\" \")))\n",
    "\n",
    "df_copy = df_copy.drop(\"sentence\", axis=1)\n",
    "\n",
    "df_exploded = df_copy.explode(\"word\").reindex()\n",
    "\n",
    "df_exploded = df_exploded.reset_index()\n",
    "\n",
    "df_grouped = df_exploded.groupby(\"word\").count().sort_values(by='index', ascending=False).reset_index()\n",
    "\n",
    "df_non_alpha = df_grouped[df_grouped['word'].apply(lambda x: not x.isalpha())]\n",
    "\n",
    "df_non_alpha.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103170"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023297"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_ngrams(df, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import create_labelled_file\n",
    "k_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "if test_locally:    \n",
    "    # Create the bag of words\n",
    "    # The token_pattern is used in order to avoid the preprocessor to remove special characters (like smiles)\n",
    "    # or hashtags (Twitter!)\n",
    "    vectorizer = CountVectorizer(token_pattern = '[a-zA-Z0-9$&+,:;=?@#|<>.^*()%!-]+')  # The vectorizer is used to create the bag of words\n",
    "\n",
    "    %time X = vectorizer.fit_transform(df['sentence'])\n",
    "    Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 102408)\n",
      "9\n",
      "dunno justin read mention justin god know hope follow #believe\n",
      "2723\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "print(X.shape)\n",
    "for x in X[0].toarray()[0]:\n",
    "    if x > 0:\n",
    "        counter += 1\n",
    "print(counter)\n",
    "print(df.iloc[0].sentence)\n",
    "print(vectorizer.vocabulary_.get('#crakkbitch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140000, 102408)\n",
      "(60000, 102408)\n",
      "112683    0\n",
      "144413    0\n",
      "70056     1\n",
      "128329    0\n",
      "184592    0\n",
      "         ..\n",
      "106638    0\n",
      "56853     1\n",
      "63980     1\n",
      "62727     1\n",
      "138761    0\n",
      "Name: label, Length: 140000, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# divide in train test split\n",
    "# CAREFUL HERE: when working with neural nets, we need to convert -1, 1 labels into 0, 1\n",
    "if test_locally:\n",
    "    train_test_split = 0.7\n",
    "    permut = np.random.permutation(X.shape[0])\n",
    "    train_x = X[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    train_y = Y[permut[: int(X.shape[0]*train_test_split)]]\n",
    "    \n",
    "    test_x = X[permut[int(X.shape[0]*train_test_split):]]\n",
    "    test_y = Y[permut[int(X.shape[0]*train_test_split):]]\n",
    "    \n",
    "    ## Convert all -1 into 0!\n",
    "    train_y = train_y.where(train_y == 1, 0) \n",
    "    test_y = test_y.where(test_y == 1, 0)\n",
    "    \n",
    "    print(train_x.shape)\n",
    "    print(test_x.shape)\n",
    "    print(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "140000/140000 [==============================] - 39s 276us/sample - loss: 0.5772 - acc: 0.7188\n",
      "Epoch 2/5\n",
      "140000/140000 [==============================] - 38s 271us/sample - loss: 0.4963 - acc: 0.7656\n",
      "Epoch 3/5\n",
      "140000/140000 [==============================] - 39s 277us/sample - loss: 0.4651 - acc: 0.7818\n",
      "Epoch 4/5\n",
      "140000/140000 [==============================] - 38s 274us/sample - loss: 0.4473 - acc: 0.7911\n",
      "Epoch 5/5\n",
      "140000/140000 [==============================] - 39s 276us/sample - loss: 0.4344 - acc: 0.7974\n",
      " - 13s - loss: 0.4465 - acc: 0.7865\n",
      "Epoch 1/5\n",
      "140000/140000 [==============================] - 39s 278us/sample - loss: 0.4243 - acc: 0.8036\n",
      "Epoch 2/5\n",
      "140000/140000 [==============================] - 39s 279us/sample - loss: 0.4153 - acc: 0.8074\n",
      "Epoch 3/5\n",
      "140000/140000 [==============================] - 39s 279us/sample - loss: 0.4071 - acc: 0.8115\n",
      "Epoch 4/5\n",
      "140000/140000 [==============================] - 40s 285us/sample - loss: 0.3995 - acc: 0.8154\n",
      "Epoch 5/5\n",
      "140000/140000 [==============================] - 40s 288us/sample - loss: 0.3924 - acc: 0.8195\n",
      " - 13s - loss: 0.4295 - acc: 0.7947\n",
      "Epoch 1/5\n",
      "140000/140000 [==============================] - 41s 292us/sample - loss: 0.3857 - acc: 0.8238\n",
      "Epoch 2/5\n",
      "140000/140000 [==============================] - 41s 294us/sample - loss: 0.3791 - acc: 0.8269\n",
      "Epoch 3/5\n",
      "140000/140000 [==============================] - 40s 288us/sample - loss: 0.3729 - acc: 0.8301\n",
      "Epoch 4/5\n",
      "140000/140000 [==============================] - 40s 287us/sample - loss: 0.3668 - acc: 0.8332\n",
      "Epoch 5/5\n",
      "140000/140000 [==============================] - 40s 289us/sample - loss: 0.3609 - acc: 0.8380\n",
      " - 13s - loss: 0.4227 - acc: 0.7994\n",
      "Epoch 1/5\n",
      "140000/140000 [==============================] - 41s 290us/sample - loss: 0.3549 - acc: 0.8404\n",
      "Epoch 2/5\n",
      "140000/140000 [==============================] - 41s 295us/sample - loss: 0.3490 - acc: 0.8437\n",
      "Epoch 3/5\n",
      " 83840/140000 [================>.............] - ETA: 16s - loss: 0.3407 - acc: 0.8497"
     ]
    }
   ],
   "source": [
    "for iter in range(5):\n",
    "    # train for 5 epochs the model \n",
    "    model.fit(train_x, train_y, epochs=5)\n",
    "    # evaluate the test error\n",
    "    model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 13s - loss: 0.4323 - acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_x,  test_y, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_locally:\n",
    "    # Do cross validation on the bag of words, using the neural network\n",
    "    df_precisions = {}\n",
    "    for epochs in tqdm(range(10, 20, 2)):\n",
    "        precisions = []\n",
    "        for k in range(k_folds):\n",
    "            \n",
    "        df_precisions[epochs] = precisions\n",
    "        print(np.array(precisions).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
