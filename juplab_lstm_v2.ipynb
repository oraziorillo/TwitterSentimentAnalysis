{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "R1DJnjjbJk7R",
    "outputId": "5fe59f75-41f8-4a56-f26a-2ebd1af8c5de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import nltk\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dz0DE0AYX3Ik"
   },
   "outputs": [],
   "source": [
    "prefix = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "SIlpVW4IQBn3",
    "outputId": "a6ee42d1-5202-480c-eae8-9968f9afdd98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/orazio/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/orazio/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S1xJOyEHJk7t"
   },
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = False\n",
    "test_locally = True\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_actions = ['clean_new_line', 'clean_tags', \\\n",
    "                    'remove_numbers', 'clean_punctuation']\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation_2,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers_2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR7IbAJCJk77"
   },
   "outputs": [],
   "source": [
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "else:\n",
    "    input_file_pos = 'Data/train_pos.txt'\n",
    "    input_file_neg = 'Data/train_neg.txt'\n",
    "    \n",
    "input_file_test = 'Data/test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xpFqg6LHJk96"
   },
   "outputs": [],
   "source": [
    "pos_sentences = []\n",
    "with open(prefix + input_file_pos, 'r') as f:\n",
    "    for sentence in f:\n",
    "        pos_sentences.append(sentence)\n",
    "        \n",
    "neg_sentences = []\n",
    "with open(prefix + input_file_neg, 'r') as f:\n",
    "    for sentence in f:\n",
    "        neg_sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = pd.DataFrame(pos_sentences, columns=['sentence'])\n",
    "pos_data['label'] = 1\n",
    "neg_data = pd.DataFrame(neg_sentences, columns=['sentence'])\n",
    "neg_data['label'] = -1\n",
    "\n",
    "data = pd.concat([pos_data, neg_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-6638fe131fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_particular_numbers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/projects/github/twitter_sentiment_analysis/clean_helpers.py\u001b[0m in \u001b[0;36mremove_particular_numbers\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    113\u001b[0m     return pd.DataFrame({\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m'sentence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr' [0123456789]+.[0123456789]+ |.[0123456789]+ | [0123456789]+.| [0123456789]+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     })\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m             )\n\u001b[1;32m    410\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         ]\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhave_series\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_union_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_union_indexes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPY36\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhave_ordered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/indexes/api.py\u001b[0m in \u001b[0;36m_union_indexes\u001b[0;34m(indexes, sort)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mother\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m   2517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_union_incompatible_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/indexes/numeric.py\u001b[0m in \u001b[0;36m_union\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_union\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_union\u001b[0;34m(self, other, sort)\u001b[0m\n\u001b[1;32m   2566\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrvals\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2568\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2569\u001b[0m             \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/programs/anaconda3/envs/twitter_sentiment_analysis/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   2983\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2984\u001b[0m             raise InvalidIndexError(\n\u001b[0;32m-> 2985\u001b[0;31m                 \u001b[0;34m\"Reindexing only valid with uniquely\"\u001b[0m \u001b[0;34m\" valued Index objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2986\u001b[0m             )\n\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "data = remove_particular_numbers(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hOtGZOQyJk-A"
   },
   "outputs": [],
   "source": [
    "for c in cleaning_actions:\n",
    "    data = clean[c](data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"clean_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ft5UYY7oJk_t"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_text_clean = ' '.join(list(data['sentence']))\n",
    "\n",
    "# Create a list of words\n",
    "words = all_text_clean.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "\n",
    "vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7YyxEFzWJk_7"
   },
   "outputs": [],
   "source": [
    "encoded_sentences = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    encoded_sentence = [vocab_to_int[w] for w in row['sentence'].split()]\n",
    "    encoded_sentences.append(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "colab_type": "code",
    "id": "glkly3tKJlAQ",
    "outputId": "8c36c65d-2eec-486d-903b-c7e69882de7d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    2.500000e+06\n",
       "mean     8.582261e+00\n",
       "std      4.578974e+00\n",
       "min      0.000000e+00\n",
       "25%      5.000000e+00\n",
       "50%      8.000000e+00\n",
       "75%      1.200000e+01\n",
       "max      5.100000e+01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_lenghts = [len(x) for x in encoded_sentences]\n",
    "max_sentence_lenght = np.max(sentences_lenghts)\n",
    "\n",
    "pd.Series(sentences_lenghts).hist()\n",
    "plt.show()\n",
    "pd.Series(sentences_lenghts).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXC9SSHoJlAe"
   },
   "outputs": [],
   "source": [
    "X = pad_sequences(encoded_sentences, maxlen=max_sentence_lenght) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "0GGJG9CdJlAt",
    "outputId": "057ad28c-5b1c-4010-d1ea-c3dafc4539b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 51, 128)           71578496  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 51, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 71,833,690\n",
      "Trainable params: 71,833,690\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab_to_int), embed_dim, mask_zero=True, input_length=max_sentence_lenght))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "XsF_5ExzJlBB",
    "outputId": "8525395b-5ec9-46e7-fc1d-a6f48c8e670b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250000, 51) (2250000, 2)\n",
      "(250000, 51) (250000, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['label']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "UZXkXBVXJlBU",
    "outputId": "087d365f-019c-48d9-ba95-249eae5744fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "2250000/2250000 [==============================] - 2376s 1ms/step - loss: 0.3900 - acc: 0.8156\n",
      "Epoch 2/5\n",
      "2250000/2250000 [==============================] - 2342s 1ms/step - loss: 0.3421 - acc: 0.8441\n",
      "Epoch 3/5\n",
      "2250000/2250000 [==============================] - 2336s 1ms/step - loss: 0.3056 - acc: 0.8642\n",
      "Epoch 4/5\n",
      "2250000/2250000 [==============================] - 2332s 1ms/step - loss: 0.2828 - acc: 0.8743\n",
      "Epoch 5/5\n",
      " 741760/2250000 [========>.....................] - ETA: 26:02 - loss: 0.2653 - acc: 0.8822"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=batch_size)\n",
    "print()\n",
    "#score,acc = model.evaluate(X_test, Y_test, verbose=2, batch_size=batch_size)\n",
    "#print(\"score: %.2f\" % (score))\n",
    "#print(\"acc: %.2f\" % (acc))\n",
    "#print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZSkCEMPJlBn"
   },
   "outputs": [],
   "source": [
    "validation_size = 10000\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pWIdHRmJlB6"
   },
   "outputs": [],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MIt_MZ_WWbik"
   },
   "outputs": [],
   "source": [
    "def prepare_test_data(list_of_sentences):    \n",
    "    test_df = build_sentences(list_of_sentences, [])\n",
    "\n",
    "    for clean_option in cleaning_options:\n",
    "        test_df = clean[clean_option](test_df)\n",
    "    \n",
    "    enc_sentences = []\n",
    "\n",
    "    for index, row in test_df.iterrows():\n",
    "        enc_sentence = []\n",
    "        \n",
    "        for w in row['sentence'].split():\n",
    "            try:\n",
    "                enc_sentence.append(vocab_to_int[w])\n",
    "            except:\n",
    "                enc_sentence.append(0)\n",
    "                \n",
    "        enc_sentences.append(enc_sentence)\n",
    "    \n",
    "    lens = [len(x) for x in enc_sentences]\n",
    "    max_len = np.max(lens)\n",
    "    x_pred = sequence.pad_sequences(enc_sentences, maxlen=max_len) \n",
    "    return x_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T4ypwExVWhx2"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jACkrkimNE3i"
   },
   "outputs": [],
   "source": [
    "test_sentences = []\n",
    "with open(input_file_test, 'r') as f:\n",
    "    for sentence in f:\n",
    "        test_sentences.append(sentence)\n",
    "        \n",
    "X_pred = prepare_test_data(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E6WlcB9wWq-s"
   },
   "outputs": [],
   "source": [
    "result = model.predict(X_pred, batch_size=1, verbose = 2)[0]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lstm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
