{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/stefano/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams, build_unique_ngrams\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import gensim   # Not sure whether it is better to use gensim or tensorflow :/\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = False\n",
    "test_locally = True\n",
    "create_new_text_files = True\n",
    "ngrams = 2\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_options = ['clean_new_line', 'remove_stopwords', 'clean_tags',\n",
    "                    'clean_punctuation', 'remove_numbers', 'lemmatize', 'remove_saxon_genitive',\n",
    "                    ]\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive,\n",
    "    \"gensim_simple\": gensim_clean   # not a good idea to use it I think! It cleans everything which is not alphabetic (special char, numbers and so on)\n",
    "}\n",
    "\n",
    "\n",
    "# algorithm_used = \"\"\n",
    "# algorithm = {\n",
    "#     \"naive_bayes\": ,\n",
    "#     \"logistic_regression\": ,\n",
    "#     \"svm\": ,\n",
    "#     \"lstm\":,\n",
    "#     \"fasttext\":,\n",
    "#     \"cnn\": ,\n",
    "# }\n",
    "\n",
    "# options = []\n",
    "# additional_options = {\n",
    "#     \"count_frequency\": ,\n",
    "#     \"count_ngrams\": ,\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_pos = 'Data/train_pos.txt'\n",
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "  \n",
    "input_file_neg = 'Data/train_neg.txt'\n",
    "if take_full:\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "    \n",
    "list_of_pos_sentences = []\n",
    "with open(input_file_pos, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_pos_sentences.append(line)\n",
    " \n",
    "list_of_neg_sentences = []\n",
    "with open(input_file_neg, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_neg_sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 114427\n"
     ]
    }
   ],
   "source": [
    "from data_handling import build_sentences\n",
    "\n",
    "df = build_sentences(list_of_pos_sentences, list_of_neg_sentences)\n",
    "\n",
    "print(\"unique words = {}\".format(count_unique_words(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 257 ms, sys: 7.95 ms, total: 264 ms\n",
      "Wall time: 264 ms\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2  \" <user> just put casper in a box ! \" looved t...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "unique words = 114427\n",
      "################################\n",
      "\n",
      "\n",
      "The number of scipy stopwords is 179\n",
      "CPU times: user 622 ms, sys: 3.63 ms, total: 626 ms\n",
      "Wall time: 626 ms\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  <user> dunno justin read mention . justin god ...      1\n",
      "1    logic dumb , even crop name photo . tsk . <url>      1\n",
      "2  \" <user> put casper box ! \" looved battle ! #c...      1\n",
      "3  <user> <user> thanks sir > > trip lil mama ......      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114257\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1.82 s, sys: 7.11 ms, total: 1.83 s\n",
      "Wall time: 1.83 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention . justin god knows ,...      1\n",
      "1          logic dumb , even crop name photo . tsk .      1\n",
      "2   \" put casper box ! \" looved battle ! #crakkbitch      1\n",
      "3  thanks sir > > trip lil mama ... keep doin ya ...      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114232\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 452 ms, sys: 0 ns, total: 452 ms\n",
      "Wall time: 451 ms\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 114214\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1.63 s, sys: 0 ns, total: 1.63 s\n",
      "Wall time: 1.63 s\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 108251\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 8.14 s, sys: 67.8 ms, total: 8.21 s\n",
      "Wall time: 8.21 s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 102624\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 420 ms, sys: 2 Âµs, total: 420 ms\n",
      "Wall time: 420 ms\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 100499\n",
      "################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dunno justin read mention justin god know hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logic dumb even crop name photo tsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>put casper box ! looved battle ! #crakkbitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thanks sir trip lil mama ... keep doin ya thang !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visiting brother tmr bestest birthday gift eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  dunno justin read mention justin god know hope...      1\n",
       "1                logic dumb even crop name photo tsk      1\n",
       "2       put casper box ! looved battle ! #crakkbitch      1\n",
       "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
       "4  visiting brother tmr bestest birthday gift eve...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "\n",
    "for clean_option in cleaning_options:\n",
    "    counter_of_occurrences = 0\n",
    "    %time df = clean[clean_option](df)\n",
    "    print(clean_option)\n",
    "    print(df.head())\n",
    "    print(\"unique words = {}\".format(count_unique_words(df)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100499"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734350"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_ngrams(df, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngrams_list = []\n",
    "for n in range(1, ngrams+1):\n",
    "    ngrams_list.extend(build_unique_ngrams(df, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834847"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100498, 734349]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_ngrams = [0 for i in range(0, ngrams+1)]\n",
    "for el in ngrams_list:\n",
    "    for i in range(1, ngrams+1):\n",
    "        if len(el.split()) == i:\n",
    "            counter_ngrams[i] += 1\n",
    "counter_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_locally:\n",
    "    train_test_split = 0.7\n",
    "    permut = np.random.permutation(df.shape[0])\n",
    "    train_x = df.iloc[permut[: int(df.shape[0]*train_test_split)]]['sentence']\n",
    "    train_y = df.iloc[permut[: int(df.shape[0]*train_test_split)]]['label']\n",
    "    test_x = df.iloc[permut[int(df.shape[0]*train_test_split): ]]['sentence']\n",
    "    test_y = df.iloc[permut[int(df.shape[0]*train_test_split): ]]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = gensim.models.Word2Vec(train_x, size=150, window=10, min_count=1, workers=8, iter=10,\n",
    "                                          sg=1)  # sg is for skip gram\n",
    "\n",
    "model_w2v.wv['computer']\n",
    "model_w2v.wv['airplane']\n",
    "model_w2v.wv['twitter']\n",
    "\n",
    "model_w2v.wv['computer'].most_similar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10, 100, 10):\n",
    "    for j in range(1, 10, 1):\n",
    "        # i iterates over the iterations for the neural net\n",
    "        # j iterates over the iterations for the word2vec model\n",
    "        model_w2v = gensim.models.Word2Vec(train_x, size=150, window=10, min_count=1, workers=8, iter=j,\n",
    "                                          sg=1)  # sg is for skip gram\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"test with {} hidden layers\".format(i))\n",
    "        model = keras.Sequential([\n",
    "            keras.layers.InputLayer(input_shape=(train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "            keras.layers.Dense(i, activation='sigmoid'),\n",
    "            keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "        ])\n",
    "        model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        best_accuracy = 0\n",
    "        for iteration in range(20):\n",
    "            model.fit(train_x, train_y, epochs=1, batch_size=16)\n",
    "            # evaluate the test error\n",
    "            if not use_partial_test:\n",
    "                acc = model.evaluate(test_x,  test_y, verbose=2)[1]\n",
    "            else:\n",
    "                permut_test = np.random.permutation(len(test_x))\n",
    "                acc = model.evaluate(test_x[permut_test[:10000]], test_y[permut_test[:10000]])\n",
    "\n",
    "            if acc < best_accuracy:\n",
    "                print(\"Overfitting, best with iter = {}\".format(iteration))\n",
    "                print(\"#################################\\n#################################\")\n",
    "                break\n",
    "            best_accuracy = acc\n",
    "\n",
    "            # save the model otherwise\n",
    "            %time model.save(\"models/model_small_{}.model\".format(i))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
