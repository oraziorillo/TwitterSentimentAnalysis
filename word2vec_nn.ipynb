{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from helpers import count_unique_words, count_unique_ngrams, \\\n",
    "            build_unique_ngrams, create_sentence_vectors, create_sentence_vectors_submission\n",
    "\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import gensim   # Not sure whether it is better to use gensim or tensorflow :/\n",
    "import logging\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_helpers import *\n",
    "\n",
    "take_full = True\n",
    "test_locally = True\n",
    "create_new_text_files = True\n",
    "ngrams = 1\n",
    "\n",
    "# Specify here what cleaning functions you want to use\n",
    "cleaning_options = ['clean_new_line', 'remove_stopwords', 'clean_tags',\n",
    "                    'clean_punctuation', 'remove_numbers', 'lemmatize', 'remove_saxon_genitive',\n",
    "                    ]\n",
    "\n",
    "\n",
    "clean = {\n",
    "    \"clean_new_line\": clean_new_line,\n",
    "    \"lowercase\": lowercase,\n",
    "    \"lemmatize\": lemmatize,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"translate\": perform_translation,\n",
    "    \"clean_punctuation\": clean_punctuation,\n",
    "    \"clean_tags\" : clean_tags,\n",
    "    \"remove_numbers\": remove_numbers,\n",
    "    \"remove_saxon_genitive\": remove_saxon_genitive,\n",
    "    \"gensim_simple\": gensim_clean   # not a good idea to use it I think! It cleans everything which is not alphabetic (special char, numbers and so on)\n",
    "}\n",
    "\n",
    "\n",
    "# algorithm_used = \"\"\n",
    "# algorithm = {\n",
    "#     \"naive_bayes\": ,\n",
    "#     \"logistic_regression\": ,\n",
    "#     \"svm\": ,\n",
    "#     \"lstm\":,\n",
    "#     \"fasttext\":,\n",
    "#     \"cnn\": ,\n",
    "# }\n",
    "\n",
    "# options = []\n",
    "# additional_options = {\n",
    "#     \"count_frequency\": ,\n",
    "#     \"count_ngrams\": ,\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_pos = 'Data/train_pos.txt'\n",
    "if take_full:\n",
    "    input_file_pos = 'Data/train_pos_full.txt'\n",
    "  \n",
    "input_file_neg = 'Data/train_neg.txt'\n",
    "if take_full:\n",
    "    input_file_neg = 'Data/train_neg_full.txt'\n",
    "    \n",
    "list_of_pos_sentences = []\n",
    "with open(input_file_pos, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_pos_sentences.append(line)\n",
    " \n",
    "list_of_neg_sentences = []\n",
    "with open(input_file_neg, 'r') as f:\n",
    "    for line in f:\n",
    "        list_of_neg_sentences.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words = 592563\n"
     ]
    }
   ],
   "source": [
    "from data_handling import build_sentences\n",
    "\n",
    "df = build_sentences(list_of_pos_sentences, list_of_neg_sentences)\n",
    "\n",
    "print(\"unique words = {}\".format(count_unique_words(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 s, sys: 107 ms, total: 4.26 s\n",
      "Wall time: 4.28 s\n",
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  <user> i dunno justin read my mention or not ....      1\n",
      "1  because your logic is so dumb , i won't even c...      1\n",
      "2  \" <user> just put casper in a box ! \" looved t...      1\n",
      "3  <user> <user> thanks sir > > don't trip lil ma...      1\n",
      "4  visiting my brother tmr is the bestest birthda...      1\n",
      "unique words = 592563\n",
      "################################\n",
      "\n",
      "\n",
      "The number of scipy stopwords is 179\n",
      "CPU times: user 9.86 s, sys: 258 ms, total: 10.1 s\n",
      "Wall time: 10.3 s\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  <user> dunno justin read mention . justin god ...      1\n",
      "1    logic dumb , even crop name photo . tsk . <url>      1\n",
      "2  \" <user> put casper box ! \" looved battle ! #c...      1\n",
      "3  <user> <user> thanks sir > > trip lil mama ......      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592388\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 31.1 s, sys: 194 ms, total: 31.3 s\n",
      "Wall time: 31.6 s\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention . justin god knows ,...      1\n",
      "1          logic dumb , even crop name photo . tsk .      1\n",
      "2   \" put casper box ! \" looved battle ! #crakkbitch      1\n",
      "3  thanks sir > > trip lil mama ... keep doin ya ...      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592239\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 6.41 s, sys: 561 Âµs, total: 6.42 s\n",
      "Wall time: 6.44 s\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 592216\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 26.1 s, sys: 7.99 ms, total: 26.1 s\n",
      "Wall time: 26.2 s\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god knows hop...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 560029\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 1min 47s, sys: 138 ms, total: 1min 47s\n",
      "Wall time: 1min 47s\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 545922\n",
      "################################\n",
      "\n",
      "\n",
      "CPU times: user 7.72 s, sys: 8.21 ms, total: 7.72 s\n",
      "Wall time: 7.81 s\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  dunno justin read mention justin god know hope...      1\n",
      "1                logic dumb even crop name photo tsk      1\n",
      "2       put casper box ! looved battle ! #crakkbitch      1\n",
      "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
      "4  visiting brother tmr bestest birthday gift eve...      1\n",
      "unique words = 533533\n",
      "################################\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>dunno justin read mention justin god know hope...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>logic dumb even crop name photo tsk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>put casper box ! looved battle ! #crakkbitch</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>thanks sir trip lil mama ... keep doin ya thang !</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>visiting brother tmr bestest birthday gift eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  dunno justin read mention justin god know hope...      1\n",
       "1                logic dumb even crop name photo tsk      1\n",
       "2       put casper box ! looved battle ! #crakkbitch      1\n",
       "3  thanks sir trip lil mama ... keep doin ya thang !      1\n",
       "4  visiting brother tmr bestest birthday gift eve...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform all the cleaning options selected\n",
    "\n",
    "for clean_option in cleaning_options:\n",
    "    counter_of_occurrences = 0\n",
    "    %time df = clean[clean_option](df)\n",
    "    print(clean_option)\n",
    "    print(df.head())\n",
    "    print(\"unique words = {}\".format(count_unique_words(df)))\n",
    "    print(\"################################\\n\\n\")\n",
    "    \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_unique_ngrams(df, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ngrams_list = []\n",
    "for n in range(1, ngrams+1):\n",
    "    ngrams_list.extend(build_unique_ngrams(df, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832786"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 100493, 732293]"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_ngrams = [0 for i in range(0, ngrams+1)]\n",
    "for el in ngrams_list:\n",
    "    for i in range(1, ngrams+1):\n",
    "        if len(el.split()) == i:\n",
    "            counter_ngrams[i] += 1\n",
    "counter_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_locally:\n",
    "    train_test_split = 0.8\n",
    "    permut = np.random.permutation(df.shape[0])\n",
    "    train_x = df.iloc[permut[: int(df.shape[0]*train_test_split)]]['sentence']\n",
    "    train_y = df.iloc[permut[: int(df.shape[0]*train_test_split)]]['label']\n",
    "    test_x = df.iloc[permut[int(df.shape[0]*train_test_split): ]]['sentence']\n",
    "    test_y = df.iloc[permut[int(df.shape[0]*train_test_split): ]]['label']\n",
    "    \n",
    "    train_y = train_y.where(train_y == 1, 0) \n",
    "    test_y = test_y.where(test_y == 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1204694    dancing stage tyga rap rack city ! word lastni...\n",
      "1763863                                  well i'm back sleep\n",
      "1573572    god know grieving thing help paperback story p...\n",
      "56768                                    sooo love pic ! ! !\n",
      "2427677                   sitting waiting email ticketek ...\n",
      "993725     smoke session room b .. girl room friday start...\n",
      "787029     good morning asking god show favor morning ble...\n",
      "219660     thank letting u know daily matter count ! cele...\n",
      "69424                                 rt i'll try sometime !\n",
      "1653757                  aye good point gcse like month away\n",
      "Name: sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_x[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [row.split() for row in train_x]\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462834\n",
      "462835\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "word_freq = defaultdict(int)\n",
    "for s in sentences:\n",
    "    for i in s:\n",
    "        word_freq[i] += 1\n",
    "print(len(word_freq))  # As we can see, the words are less than the original ones.\n",
    "print(count_unique_words(df.iloc[permut[: int(df.shape[0]*train_test_split)]]))\n",
    "\n",
    "# The discrepancy among the two might be due to the nan? Actually shouldn't make much difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '...', '?', \"i'm\", 'rt', 'love', 'u', 'like', '..', 'get']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vector_size = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.level = logging.ERROR   # Should reduce logging\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(min_count=1,\n",
    "                     window=10,\n",
    "                     size=word_vector_size,\n",
    "                     negative=5,\n",
    "                     workers=4,\n",
    "                     sg=1)    ## Careful here: it should work better with sg=1 for big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences, progress_per=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119500488, 133938192)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"models/word2vec/300_8_epochs_sg_neg_5_win_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-12 13:45:26,765 : INFO : loading Word2Vec object from models/word2vec/300_8_epochs_sg_neg_5_win_10.model\n",
      "2019-12-12 13:45:27,351 : INFO : loading wv recursively from models/word2vec/300_8_epochs_sg_neg_5_win_10.model.wv.* with mmap=None\n",
      "2019-12-12 13:45:27,351 : INFO : loading vectors from models/word2vec/300_8_epochs_sg_neg_5_win_10.model.wv.vectors.npy with mmap=None\n",
      "2019-12-12 13:45:28,643 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-12-12 13:45:28,644 : INFO : loading vocabulary recursively from models/word2vec/300_8_epochs_sg_neg_5_win_10.model.vocabulary.* with mmap=None\n",
      "2019-12-12 13:45:28,649 : INFO : loading trainables recursively from models/word2vec/300_8_epochs_sg_neg_5_win_10.model.trainables.* with mmap=None\n",
      "2019-12-12 13:45:28,653 : INFO : loading syn1neg from models/word2vec/300_8_epochs_sg_neg_5_win_10.model.trainables.syn1neg.npy with mmap=None\n",
      "2019-12-12 13:45:29,798 : INFO : setting ignored attribute cum_table to None\n",
      "2019-12-12 13:45:29,798 : INFO : loaded models/word2vec/300_8_epochs_sg_neg_5_win_10.model\n"
     ]
    }
   ],
   "source": [
    "w2v_model = Word2Vec.load(\"models/word2vec/300_8_epochs_sg_neg_5_win_10.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87485"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"#i'mabelieber\", 0.6168533563613892),\n",
       " ('france', 0.5785192251205444),\n",
       " ('1dx', 0.5747607350349426),\n",
       " ('ciaooo', 0.5639715790748596),\n",
       " ('ahhaahah', 0.5627542734146118),\n",
       " ('bulgaria', 0.5615280866622925),\n",
       " ('1dxxx', 0.5610278248786926),\n",
       " ('portugal', 0.5598273873329163),\n",
       " ('estonia', 0.5575746297836304),\n",
       " ('lithuania', 0.5546209812164307)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(\"italy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.4 s, sys: 4.76 s, total: 47.2 s\n",
      "Wall time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Here we create the sentences, by averaging the word vectors in each sentence.\n",
    "sentence_train_x, sentence_train_y = create_sentence_vectors(train_x[:500000], train_y[:500000], word_vector_size, w2v_model)\n",
    "sentence_test_x, sentence_test_y = create_sentence_vectors(test_x[:500000], test_y[:500000], word_vector_size, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?   y\n"
     ]
    }
   ],
   "source": [
    "%reset_selective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(499117, 300)\n",
      "(499117, 2)\n",
      "(499157, 300)\n",
      "(499157, 2)\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_train_x.shape)\n",
    "print(sentence_train_y.shape)\n",
    "print(sentence_test_x.shape)\n",
    "print(sentence_test_y.shape)\n",
    "# print(sentence_train_x[:2])\n",
    "print(sentence_train_y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Starting with number of layers: 15\n",
      "499117/499117 [==============================] - 31s 63us/sample - loss: 0.4369 - acc: 0.7900\n",
      " - 10s - loss: 0.4289 - acc: 0.7932\n",
      "499117/499117 [==============================] - 31s 61us/sample - loss: 0.4208 - acc: 0.7995\n",
      " - 9s - loss: 0.4231 - acc: 0.7977\n",
      "499117/499117 [==============================] - 31s 62us/sample - loss: 0.4151 - acc: 0.8020\n",
      " - 10s - loss: 0.4192 - acc: 0.7998\n",
      "499117/499117 [==============================] - 31s 62us/sample - loss: 0.4117 - acc: 0.8041\n",
      " - 9s - loss: 0.4156 - acc: 0.8015\n",
      "499117/499117 [==============================] - 32s 63us/sample - loss: 0.4096 - acc: 0.8055\n",
      " - 9s - loss: 0.4125 - acc: 0.8031\n",
      "499117/499117 [==============================] - 32s 64us/sample - loss: 0.4077 - acc: 0.8065\n",
      " - 9s - loss: 0.4127 - acc: 0.8032\n",
      "499117/499117 [==============================] - 33s 65us/sample - loss: 0.4065 - acc: 0.8068\n",
      " - 9s - loss: 0.4198 - acc: 0.7990\n",
      "499117/499117 [==============================] - 33s 66us/sample - loss: 0.4053 - acc: 0.8079\n",
      " - 10s - loss: 0.4120 - acc: 0.8039\n",
      "499117/499117 [==============================] - 33s 67us/sample - loss: 0.4042 - acc: 0.8083\n",
      " - 9s - loss: 0.4117 - acc: 0.8043\n",
      "499117/499117 [==============================] - 34s 68us/sample - loss: 0.4037 - acc: 0.8085\n",
      " - 9s - loss: 0.4125 - acc: 0.8030\n",
      "499117/499117 [==============================] - 34s 69us/sample - loss: 0.4030 - acc: 0.8091\n",
      " - 9s - loss: 0.4124 - acc: 0.8030\n",
      "499117/499117 [==============================] - 34s 68us/sample - loss: 0.4026 - acc: 0.8090\n",
      " - 9s - loss: 0.4120 - acc: 0.8039\n",
      "###############################\n",
      "###############################\n",
      "Overfitting, best accuracy with 0.8042579889297485\n",
      "\n",
      "\n",
      "Starting with number of layers: 20\n",
      "499117/499117 [==============================] - 36s 72us/sample - loss: 0.4361 - acc: 0.7897\n",
      " - 10s - loss: 0.4251 - acc: 0.7963\n",
      "499117/499117 [==============================] - 36s 72us/sample - loss: 0.4197 - acc: 0.7995\n",
      " - 10s - loss: 0.4186 - acc: 0.8000\n",
      "499117/499117 [==============================] - 35s 71us/sample - loss: 0.4136 - acc: 0.8032\n",
      " - 10s - loss: 0.4138 - acc: 0.8026\n",
      "499117/499117 [==============================] - 37s 73us/sample - loss: 0.4097 - acc: 0.8054\n",
      " - 10s - loss: 0.4159 - acc: 0.8012\n",
      "499117/499117 [==============================] - 37s 75us/sample - loss: 0.4069 - acc: 0.8072\n",
      " - 10s - loss: 0.4173 - acc: 0.8007\n",
      "499117/499117 [==============================] - 37s 75us/sample - loss: 0.4047 - acc: 0.8080\n",
      " - 10s - loss: 0.4112 - acc: 0.8036\n",
      "499117/499117 [==============================] - 38s 76us/sample - loss: 0.4033 - acc: 0.8090\n",
      " - 10s - loss: 0.4112 - acc: 0.8047\n",
      "499117/499117 [==============================] - 38s 76us/sample - loss: 0.4020 - acc: 0.8098\n",
      " - 10s - loss: 0.4126 - acc: 0.8041\n",
      "499117/499117 [==============================] - 1594s 3ms/sample - loss: 0.4009 - acc: 0.8105\n",
      " - 10s - loss: 0.4118 - acc: 0.8038\n",
      "499117/499117 [==============================] - 39s 79us/sample - loss: 0.4002 - acc: 0.8107\n",
      " - 10s - loss: 0.4088 - acc: 0.8059\n",
      "499117/499117 [==============================] - 39s 79us/sample - loss: 0.3993 - acc: 0.8112\n",
      " - 10s - loss: 0.4092 - acc: 0.8051\n",
      "499117/499117 [==============================] - 39s 79us/sample - loss: 0.3986 - acc: 0.8117\n",
      " - 11s - loss: 0.4093 - acc: 0.8055\n",
      "499117/499117 [==============================] - 40s 79us/sample - loss: 0.3980 - acc: 0.8117\n",
      " - 10s - loss: 0.4108 - acc: 0.8048\n",
      "###############################\n",
      "###############################\n",
      "Overfitting, best accuracy with 0.8058807253837585\n",
      "\n",
      "\n",
      "Starting with number of layers: 25\n",
      "499117/499117 [==============================] - 33s 66us/sample - loss: 0.4339 - acc: 0.7912\n",
      " - 11s - loss: 0.4253 - acc: 0.7951\n",
      "499117/499117 [==============================] - 30s 60us/sample - loss: 0.4170 - acc: 0.8009\n",
      " - 11s - loss: 0.4144 - acc: 0.8020\n",
      "499117/499117 [==============================] - 34s 68us/sample - loss: 0.4104 - acc: 0.8048\n",
      " - 11s - loss: 0.4170 - acc: 0.8009\n",
      "499117/499117 [==============================] - 28s 56us/sample - loss: 0.4062 - acc: 0.8072\n",
      " - 12s - loss: 0.4105 - acc: 0.8044\n",
      "499117/499117 [==============================] - 34s 69us/sample - loss: 0.4027 - acc: 0.8091\n",
      " - 14s - loss: 0.4094 - acc: 0.8054\n",
      "499117/499117 [==============================] - 32s 64us/sample - loss: 0.4007 - acc: 0.8104\n",
      " - 11s - loss: 0.4103 - acc: 0.8041\n",
      "499117/499117 [==============================] - 34s 69us/sample - loss: 0.3985 - acc: 0.8114\n",
      " - 18s - loss: 0.4074 - acc: 0.8065\n",
      "499117/499117 [==============================] - 33s 66us/sample - loss: 0.3971 - acc: 0.8121\n",
      " - 13s - loss: 0.4061 - acc: 0.8066\n",
      "499117/499117 [==============================] - 30s 61us/sample - loss: 0.3959 - acc: 0.8130\n",
      " - 13s - loss: 0.4113 - acc: 0.8029\n",
      "499117/499117 [==============================] - 30s 60us/sample - loss: 0.3947 - acc: 0.8138\n",
      " - 11s - loss: 0.4064 - acc: 0.8063\n",
      "499117/499117 [==============================] - 30s 59us/sample - loss: 0.3935 - acc: 0.8146\n",
      " - 11s - loss: 0.4063 - acc: 0.8070\n",
      "499117/499117 [==============================] - 31s 63us/sample - loss: 0.3928 - acc: 0.8144\n",
      " - 11s - loss: 0.4050 - acc: 0.8071\n",
      "499117/499117 [==============================] - 29s 57us/sample - loss: 0.3919 - acc: 0.8148\n",
      " - 11s - loss: 0.4072 - acc: 0.8074\n",
      "499117/499117 [==============================] - 28s 57us/sample - loss: 0.3913 - acc: 0.8154\n",
      " - 10s - loss: 0.4057 - acc: 0.8074\n",
      "499117/499117 [==============================] - 28s 56us/sample - loss: 0.3905 - acc: 0.8159\n",
      " - 13s - loss: 0.4079 - acc: 0.8069\n",
      "499117/499117 [==============================] - 39s 79us/sample - loss: 0.3899 - acc: 0.8159\n",
      " - 11s - loss: 0.4081 - acc: 0.8059\n",
      "###############################\n",
      "###############################\n",
      "Overfitting, best accuracy with 0.8074032664299011\n",
      "\n",
      "\n",
      "Starting with number of layers: 30\n",
      "499117/499117 [==============================] - 37s 74us/sample - loss: 0.4337 - acc: 0.7913\n",
      " - 13s - loss: 0.4247 - acc: 0.7959\n",
      "499117/499117 [==============================] - 29s 58us/sample - loss: 0.4158 - acc: 0.8016\n",
      " - 12s - loss: 0.4166 - acc: 0.8009\n",
      "499117/499117 [==============================] - 31s 63us/sample - loss: 0.4092 - acc: 0.8053\n",
      " - 12s - loss: 0.4128 - acc: 0.8026\n",
      "499117/499117 [==============================] - 30s 59us/sample - loss: 0.4052 - acc: 0.8073\n",
      " - 11s - loss: 0.4152 - acc: 0.8017\n",
      "499117/499117 [==============================] - 29s 58us/sample - loss: 0.4020 - acc: 0.8093\n",
      " - 10s - loss: 0.4091 - acc: 0.8050\n",
      "499117/499117 [==============================] - 29s 59us/sample - loss: 0.3995 - acc: 0.8106\n",
      " - 11s - loss: 0.4088 - acc: 0.8058\n",
      "499117/499117 [==============================] - 30s 61us/sample - loss: 0.3978 - acc: 0.8114\n",
      " - 12s - loss: 0.4125 - acc: 0.8037\n",
      "499117/499117 [==============================] - 29s 59us/sample - loss: 0.3959 - acc: 0.8122\n",
      " - 12s - loss: 0.4045 - acc: 0.8072\n",
      "499117/499117 [==============================] - 38s 77us/sample - loss: 0.3946 - acc: 0.8133\n",
      " - 11s - loss: 0.4066 - acc: 0.8062\n",
      "499117/499117 [==============================] - 36s 71us/sample - loss: 0.3933 - acc: 0.8138\n",
      " - 14s - loss: 0.4067 - acc: 0.8076\n",
      "499117/499117 [==============================] - 28s 56us/sample - loss: 0.3921 - acc: 0.8140\n",
      " - 12s - loss: 0.4054 - acc: 0.8071\n",
      "499117/499117 [==============================] - 29s 59us/sample - loss: 0.3913 - acc: 0.8148\n",
      " - 13s - loss: 0.4045 - acc: 0.8079\n",
      "499117/499117 [==============================] - 30s 60us/sample - loss: 0.3903 - acc: 0.8152\n",
      " - 11s - loss: 0.4162 - acc: 0.7994\n",
      "499117/499117 [==============================] - 28s 57us/sample - loss: 0.3893 - acc: 0.8161\n",
      " - 10s - loss: 0.4045 - acc: 0.8078\n",
      "499117/499117 [==============================] - 30s 60us/sample - loss: 0.3888 - acc: 0.8164\n",
      " - 13s - loss: 0.4063 - acc: 0.8070\n",
      "###############################\n",
      "###############################\n",
      "Overfitting, best accuracy with 0.807852029800415\n"
     ]
    }
   ],
   "source": [
    "# now perform training on the new features vectors.\n",
    "\n",
    "# Build a \"deep\" neural network with 2 hidden layers. When we see that it somehow works,\n",
    "# we can start doing some cross validation on it.\n",
    "\n",
    "for layer_size in range(15, 31, 5):\n",
    "    \n",
    "    print(\"\\n\\nStarting with number of layers: {}\".format(layer_size))\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(sentence_train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "        keras.layers.Dense(layer_size, activation='relu'),\n",
    "        keras.layers.Dense(layer_size, activation='relu'),\n",
    "        keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    overfitting_occurrences = 0\n",
    "    best_accuracy = 0\n",
    "    for iteration in range(50):\n",
    "        \n",
    "        model.fit(x=sentence_train_x,\n",
    "                  y=sentence_train_y,\n",
    "                  validation_data=(sentence_test_x,  sentence_test_y),\n",
    "                  epochs=1, use_multiprocessing=True)\n",
    "        # evaluate the test error\n",
    "        acc = model.evaluate(sentence_test_x,  sentence_test_y, verbose=2)[1]\n",
    "        if acc < best_accuracy:\n",
    "            overfitting_occurrences += 1\n",
    "            if overfitting_occurrences > 2:\n",
    "                print(\"###############################\")\n",
    "                print(\"###############################\")\n",
    "                print(\"Overfitting, best accuracy with {}\".format(best_accuracy))\n",
    "                break\n",
    "        else:\n",
    "            overfitting_occurrences = 0\n",
    "            best_accuracy = acc\n",
    "\n",
    "            # save the model otherwise\n",
    "            model.save(\"models/model_sentence_rep_small_{}.model\".format(layer_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_star = keras.Sequential([\n",
    "        keras.layers.InputLayer(input_shape=(sentence_train_x.shape[1],)),   # the input shape is the number of words in the bow dictionary\n",
    "        keras.layers.Dense(layer_size, activation='relu'),\n",
    "        keras.layers.Dense(layer_size, activation='relu'),\n",
    "        keras.layers.Dense(2, activation='softmax')   # Only 0 and 1\n",
    "    ])\n",
    "\n",
    "model_star.load_weights('models/model_sentence_rep_small_30.model')\n",
    "\n",
    "model_star.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7ffa408c1610>\n"
     ]
    }
   ],
   "source": [
    "print(model_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 11s - loss: 0.4045 - acc: 0.8079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4.0681679e-02, 9.5931840e-01],\n",
       "       [9.9999571e-01, 4.2639813e-06],\n",
       "       [7.2958276e-02, 9.2704177e-01],\n",
       "       ...,\n",
       "       [3.4315959e-01, 6.5684032e-01],\n",
       "       [6.1612552e-01, 3.8387451e-01],\n",
       "       [6.4644866e-02, 9.3535513e-01]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_star.evaluate(sentence_test_x,  sentence_test_y, verbose=2)[1]\n",
    "model_star.predict(sentence_test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sea doo pro sea scooter ( sports with the port...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;user&gt; shucks well i work all week so now i ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>i cant stay away from bug thats my baby\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;user&gt; no ma'am ! ! ! lol im perfectly fine an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>whenever i fall asleep watching the tv , i alw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           sentence\n",
       "0      1  sea doo pro sea scooter ( sports with the port...\n",
       "1      2  <user> shucks well i work all week so now i ca...\n",
       "2      3          i cant stay away from bug thats my baby\\n\n",
       "3      4  <user> no ma'am ! ! ! lol im perfectly fine an...\n",
       "4      5  whenever i fall asleep watching the tv , i alw..."
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We have to get the test dataset and clean it as we have done with the training dataset\n",
    "df_test = []\n",
    "with open(\"Data/test_data.txt\", 'r') as f:\n",
    "    for l in f:\n",
    "        id_ = l.split(\",\")[0]\n",
    "        # it is a csv, but you have to keep other commas (only the first one is relevant)\n",
    "        sentence = \",\".join(l.split(\",\")[1:])\n",
    "        df_test.append({\n",
    "            \"label\": int(id_),\n",
    "            \"sentence\": sentence\n",
    "        })\n",
    "df_test = pd.DataFrame(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_new_line\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter ( sports with the port...      1\n",
      "1  <user> shucks well i work all week so now i ca...      2\n",
      "2            i cant stay away from bug thats my baby      3\n",
      "3  <user> no ma'am ! ! ! lol im perfectly fine an...      4\n",
      "4  whenever i fall asleep watching the tv , i alw...      5\n",
      "################################\n",
      "\n",
      "\n",
      "The number of scipy stopwords is 179\n",
      "remove_stopwords\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter ( sports portable sea-...      1\n",
      "1  <user> shucks well work week can't come cheer ...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  <user> ma'am ! ! ! lol im perfectly fine conta...      4\n",
      "4  whenever fall asleep watching tv , always wake...      5\n",
      "################################\n",
      "\n",
      "\n",
      "clean_tags\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter ( sports portable sea-...      1\n",
      "1  shucks well work week can't come cheer ! oh pu...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  ma'am ! ! ! lol im perfectly fine contagious a...      4\n",
      "4  whenever fall asleep watching tv , always wake...      5\n",
      "################################\n",
      "\n",
      "\n",
      "clean_punctuation\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter sports portable sea-do...      1\n",
      "1  shucks well work week can't come cheer ! oh pu...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  ma'am ! ! ! lol im perfectly fine contagious a...      4\n",
      "4  whenever fall asleep watching tv always wake h...      5\n",
      "################################\n",
      "\n",
      "\n",
      "remove_numbers\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter sports portable sea-do...      1\n",
      "1  shucks well work week can't come cheer ! oh pu...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  ma'am ! ! ! lol im perfectly fine contagious a...      4\n",
      "4  whenever fall asleep watching tv always wake h...      5\n",
      "################################\n",
      "\n",
      "\n",
      "lemmatize\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter sport portable sea-doo...      1\n",
      "1  shuck well work week can't come cheer ! oh put...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  ma'am ! ! ! lol im perfectly fine contagious a...      4\n",
      "4  whenever fall asleep watching tv always wake h...      5\n",
      "################################\n",
      "\n",
      "\n",
      "remove_saxon_genitive\n",
      "                                            sentence  label\n",
      "0  sea doo pro sea scooter sport portable sea-doo...      1\n",
      "1  shuck well work week can't come cheer ! oh put...      2\n",
      "2                      cant stay away bug thats baby      3\n",
      "3  ma'am ! ! ! lol im perfectly fine contagious a...      4\n",
      "4  whenever fall asleep watching tv always wake h...      5\n",
      "################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for clean_option in cleaning_options:\n",
    "        df_test = clean[clean_option](df_test)\n",
    "        print(clean_option)\n",
    "        print(df_test.head())\n",
    "        print(\"################################\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of zero sentences (the sentences which have 0 words in our vocabulary) is 22\n"
     ]
    }
   ],
   "source": [
    "sentence_submission_x = create_sentence_vectors_submission(df_test['sentence'],\n",
    "                                                           word_vector_size,\n",
    "                                                           w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96396494, 0.03603501],\n",
       "       [0.6632302 , 0.33676985],\n",
       "       [0.7554848 , 0.24451518],\n",
       "       ...,\n",
       "       [0.9986198 , 0.00138015],\n",
       "       [0.05644046, 0.9435596 ],\n",
       "       [0.9798126 , 0.02018739]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_star.predict(sentence_submission_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, 1, -1, -1, -1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id  Prediction\n",
       "0    1          -1\n",
       "1    2          -1\n",
       "2    3          -1\n",
       "3    4           1\n",
       "4    5          -1\n",
       "5    6          -1\n",
       "6    7          -1\n",
       "7    8           1\n",
       "8    9           1\n",
       "9   10           1\n",
       "10  11           1\n",
       "11  12           1\n",
       "12  13           1\n",
       "13  14          -1\n",
       "14  15           1\n",
       "15  16           1\n",
       "16  17          -1\n",
       "17  18           1\n",
       "18  19           1\n",
       "19  20          -1"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "for el in model_star.predict(sentence_submission_x):\n",
    "    predictions.append(-1 if el[0] > el[1] else 1)\n",
    "\n",
    "print(predictions[:10])\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Id\": df_test['label'],\n",
    "    \"Prediction\": predictions\n",
    "})\n",
    "\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv('Submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
